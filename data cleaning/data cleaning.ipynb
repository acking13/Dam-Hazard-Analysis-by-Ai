{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc196376-04f1-4b8f-828c-b70a65c6e0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from '..//Dam Incident Database Search  Association of State Dam Safety.csv'...\n",
      "Original dataset contains 1344 rows.\n",
      "\n",
      "Columns with NO empty cells found:\n",
      "- Dam Namex\n",
      "- StateSelect valueAKARAZCACOCTDEFLGAHIIAIDILINKYLAMAMDMEMIMNMOMSMTNCNDNENHNJNMNVNYOHOKORPARISCSDTNTXUTVAVTVT00000WAWIWVWYx\n",
      "- Incident Datex\n",
      "- Incident DriverSelect valueDeterioration or Poor ConditionHydrologic /FloodingHydrologic/FloodingMalfunction of Equipment/GateManmade ActionOtherSeepage/Internal ErosionSeismicStructural StabilityUnknownx\n",
      "- Incident ID\n",
      "- Surface Area (acres)\n",
      "- NID Number\n",
      "- Latitude\n",
      "- Longitude\n",
      "------------------------------\n",
      "Removed 1344 rows with empty cells.\n",
      "Cleaned dataset now contains 0 rows.\n",
      "Successfully saved the cleaned data to 'cleaned_dam_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_dataset(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Reads a dataset, identifies columns without empty cells,\n",
    "    removes rows with any empty cells, and saves the cleaned data.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): The path to the input CSV file.\n",
    "        output_filepath (str): The path where the cleaned CSV file will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the dataset from the specified CSV file\n",
    "        # The separator has been changed to a comma (',') to handle CSV files.\n",
    "        print(f\"Reading data from '{input_filepath}'...\")\n",
    "        df = pd.read_csv(input_filepath, sep=',')\n",
    "\n",
    "        # Get the original number of rows for comparison\n",
    "        original_rows = len(df)\n",
    "        print(f\"Original dataset contains {original_rows} rows.\")\n",
    "\n",
    "        # --- New: Find and report columns with no empty cells ---\n",
    "        # A list comprehension checks each column for any null values.\n",
    "        columns_without_empty_cells = [col for col in df.columns if not df[col].isnull().any()]\n",
    "\n",
    "        if columns_without_empty_cells:\n",
    "            print(\"\\nColumns with NO empty cells found:\")\n",
    "            for col in columns_without_empty_cells:\n",
    "                print(f\"- {col}\")\n",
    "        else:\n",
    "            print(\"\\nAll columns have at least one empty cell.\")\n",
    "        print(\"-\" * 30) # Add a separator for better readability\n",
    "        # --- End of new section ---\n",
    "\n",
    "        # Drop rows where at least one element is missing.\n",
    "        # The .dropna() method handles this automatically.\n",
    "        cleaned_df = df.dropna()\n",
    "\n",
    "        # Get the new number of rows\n",
    "        cleaned_rows = len(cleaned_df)\n",
    "        rows_removed = original_rows - cleaned_rows\n",
    "        print(f\"Removed {rows_removed} rows with empty cells.\")\n",
    "        print(f\"Cleaned dataset now contains {cleaned_rows} rows.\")\n",
    "\n",
    "        # Save the cleaned dataframe to a new CSV file\n",
    "        # The separator is also set to a comma for the output file.\n",
    "        cleaned_df.to_csv(output_filepath, index=False, sep=',')\n",
    "        print(f\"Successfully saved the cleaned data to '{output_filepath}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_filepath}' was not found. Please make sure it's in the correct directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # Define the name of your input data file.\n",
    "    # Updated to match the filename from the error message.\n",
    "    input_file = \"..//Dam Incident Database Search  Association of State Dam Safety.csv\"\n",
    "\n",
    "    # Define the name for the output file that will contain the cleaned data.\n",
    "    output_file = \"cleaned_dam_data.csv\"\n",
    "    # --- End Configuration ---\n",
    "\n",
    "    # Run the cleaning function\n",
    "    clean_dataset(input_file, output_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec907c44-02fb-4493-8ad2-8d996ec51627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26874f5f-bc34-43a1-bd77-0fa385f42a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from '..//Dam Incident Database Search  Association of State Dam Safety.csv'...\n",
      "Original dataset contains 1344 rows and 41 columns.\n",
      "\n",
      "--- Cleaning Column Names ---\n",
      "Original names -> Cleaned names:\n",
      "'Dam Namex' -> 'dam_name'\n",
      "'StateSelect valueAKARAZCACOCTDEFLGAHIIAIDILINKYLAMAMDMEMIMNMOMSMTNCNDNENHNJNMNVNYOHOKORPARISCSDTNTXUTVAVTVT00000WAWIWVWYx' -> 'state'\n",
      "'Downstream Hazard PotentialSelect valueHLSUx' -> 'downstream_hazard_potential'\n",
      "'Incident TypeSelect valueFailureNon-Failurex' -> 'incident_type'\n",
      "'Incident Datex' -> 'incident_date'\n",
      "'Incident DriverSelect valueDeterioration or Poor ConditionHydrologic /FloodingHydrologic/FloodingMalfunction of Equipment/GateManmade ActionOtherSeepage/Internal ErosionSeismicStructural StabilityUnknownx' -> 'incident_driver'\n",
      "'Incident Mechanism 1Select valueAbutment FailureAccident Or MisoperationAnimal ActivityCrackingDebris CloggingErosionErosion Of Spillway ChuteExcessive/Increased SeepageFoundation DeficiencyGate/Valve FailureHigh Reservoir LevelInsufficient Spillway CapacityLandslideOtherOvertoppingPipingReservoir OverfillingSettlementSinkholeSlidingSlope StabilitySpillway Chute FailureSpillway DeficiencySpillway Erosion/Head CuttingSpillway Pipe FailureSpillway Riser FailureUnder InvestigationUnknownVegetationx' -> 'incident_mechanism_1'\n",
      "'Incident Mechanism 2' -> 'incident_mechanism_2'\n",
      "'Incident Mechanism 3' -> 'incident_mechanism_3'\n",
      "'Incident ID' -> 'incident_id'\n",
      "'Incident Description' -> 'incident_description'\n",
      "'Named Hydrologic Event' -> 'named_hydrologic_event'\n",
      "'EAP Enacted (Y/N) due to Incident' -> 'eap_enacted_y_n_due_to_incident'\n",
      "'Fatalities (Number)' -> 'fatalities_number'\n",
      "'Number of People Evacuated' -> 'number_of_people_evacuated'\n",
      "'Number of Habitable Structures Evacuated' -> 'number_of_habitable_structures_evacuated'\n",
      "'Number of Habitable Structures Flooded' -> 'number_of_habitable_structures_flooded'\n",
      "'Other Infrastructure Impacts' -> 'other_infrastructure_impacts'\n",
      "'Economic Damages (in $)' -> 'economic_damages_in'\n",
      "'Response' -> 'response'\n",
      "'Volume released at failure (ac-ft)' -> 'volume_released_at_failure_ac_ft'\n",
      "'Additional Remarks or Updates' -> 'additional_remarks_or_updates'\n",
      "'Owner Type' -> 'owner_type'\n",
      "'Dam Type' -> 'dam_type'\n",
      "'Primary Purpose(s)' -> 'primary_purpose_s'\n",
      "'EAP' -> 'eap'\n",
      "'Dam Height' -> 'dam_height'\n",
      "'Max Storage (ac-ft)' -> 'max_storage_ac_ft'\n",
      "'Surface Area (acres)' -> 'surface_area_acres'\n",
      "'Year Completed' -> 'year_completed'\n",
      "'NID Number' -> 'nid_number'\n",
      "'River Name' -> 'river_name'\n",
      "'Latitude' -> 'latitude'\n",
      "'Longitude' -> 'longitude'\n",
      "'Year Modified' -> 'year_modified'\n",
      "'Regulatory Agency(ies)' -> 'regulatory_agency_ies'\n",
      "'Incident Time' -> 'incident_time'\n",
      "'Incident Duration' -> 'incident_duration'\n",
      "'Incident Report Produced' -> 'incident_report_produced'\n",
      "'Information Sources' -> 'information_sources'\n",
      "'Attachments' -> 'attachments'\n",
      "\n",
      "--- Filling Empty Cells ---\n",
      "Found and filled 21109 empty cells with '-1'.\n",
      "\n",
      "Successfully saved the processed data to 'processed_dam_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_dataset(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Reads a dataset, cleans its column names, fills any empty cells with -1,\n",
    "    and saves the processed data to a new file.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): The path to the input CSV file.\n",
    "        output_filepath (str): The path where the processed CSV file will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the dataset from the specified CSV file\n",
    "        print(f\"Reading data from '{input_filepath}'...\")\n",
    "        df = pd.read_csv(input_filepath, sep=',')\n",
    "        print(f\"Original dataset contains {len(df)} rows and {len(df.columns)} columns.\")\n",
    "\n",
    "        # --- New: Clean Column Names ---\n",
    "        print(\"\\n--- Cleaning Column Names ---\")\n",
    "        original_columns = df.columns.tolist()\n",
    "\n",
    "        cleaned_columns = []\n",
    "        for col in original_columns:\n",
    "            new_col = str(col).strip()\n",
    "            # Remove specific suffixes like 'x' and 'Select value...' from this dataset\n",
    "            if new_col.endswith('x'):\n",
    "                new_col = new_col[:-1]\n",
    "            if 'Select value' in new_col:\n",
    "                new_col = new_col.split('Select value')[0].strip()\n",
    "\n",
    "            # General cleaning for easier use in code\n",
    "            new_col = new_col.lower()  # Convert to lowercase\n",
    "            new_col = new_col.replace(' ', '_')  # Replace spaces with underscores\n",
    "            new_col = re.sub(r'[()./$-]', '_', new_col) # Replace special characters with underscores\n",
    "            new_col = re.sub(r'_+', '_', new_col) # Consolidate multiple underscores\n",
    "            new_col = new_col.strip('_') # Remove any trailing underscore\n",
    "            cleaned_columns.append(new_col)\n",
    "\n",
    "        df.columns = cleaned_columns\n",
    "\n",
    "        print(\"Original names -> Cleaned names:\")\n",
    "        for orig, clean in zip(original_columns, cleaned_columns):\n",
    "            if orig != clean:\n",
    "                print(f\"'{orig}' -> '{clean}'\")\n",
    "        # --- End of column cleaning ---\n",
    "\n",
    "\n",
    "        # --- Modified: Fill empty cells instead of dropping rows ---\n",
    "        print(\"\\n--- Filling Empty Cells ---\")\n",
    "        empty_cells_count = df.isnull().sum().sum()\n",
    "\n",
    "        if empty_cells_count > 0:\n",
    "            print(f\"Found and filled {empty_cells_count} empty cells with '-1'.\")\n",
    "            processed_df = df.fillna(-1)\n",
    "        else:\n",
    "            print(\"No empty cells were found.\")\n",
    "            processed_df = df # No changes needed if there are no empty cells\n",
    "        # --- End of modification ---\n",
    "\n",
    "\n",
    "        # Save the processed dataframe to a new CSV file\n",
    "        processed_df.to_csv(output_filepath, index=False, sep=',')\n",
    "        print(f\"\\nSuccessfully saved the processed data to '{output_filepath}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_filepath}' was not found. Please make sure it's in the correct directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # Define the name of your input data file.\n",
    "    input_file = \"..//Dam Incident Database Search  Association of State Dam Safety.csv\"\n",
    "\n",
    "    # Define the name for the output file that will contain the processed data.\n",
    "    output_file = \"processed_dam_data.csv\"\n",
    "    # --- End Configuration ---\n",
    "\n",
    "    # Run the processing function\n",
    "    process_dataset(input_file, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54680ba-2f48-42d8-8f6b-d3f7cdd927fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
