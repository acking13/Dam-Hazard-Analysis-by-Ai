{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0b27b-74ea-4b83-98d3-b44f2958ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the frist classifier model that work write but dont give us the resualt format we want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f17c907-29a0-4605-95fb-14a1f0ff23c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing target: incident_type ---\n",
      "Warning for target 'incident_type': Removing classes with only 1 sample: [2]\n",
      "Removed 1 rows.\n",
      "Applying SMOTE... Using k_neighbors=5.\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Training the model...\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as incident_type.h5\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "Confusion matrix plot saved as confusion_matrix_incident_type.svg\n",
      "✅ Added binary classification results for 'incident_type' to summary.\n",
      "Detailed report saved as report_incident_type.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_1 ---\n",
      "Warning for target 'incident_mechanism_1': Removing classes with only 1 sample: [27, 28, 29]\n",
      "Removed 3 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as incident_mechanism_1.h5\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_1.svg\n",
      "ℹ️ Skipping summary for 'incident_mechanism_1' (not a binary classification).\n",
      "Detailed report saved as report_incident_mechanism_1.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_2 ---\n",
      "Warning for target 'incident_mechanism_2': Removing classes with only 1 sample: [13, 22, 23, 26, 27]\n",
      "Removed 5 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as incident_mechanism_2.h5\n",
      "9/9 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x000001E5CD76B920> (for post_execute):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "import os\n",
    "\n",
    "# Try to import imblearn, provide install instructions if it fails\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except ImportError:\n",
    "    print(\"Error: The 'imbalanced-learn' library is required but not installed.\")\n",
    "    print(\"Please install it by running the following command in your terminal:\")\n",
    "    print(\"pip install imbalanced-learn\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# Using the pre-processed classifier data file\n",
    "DATA_FILE = '../classifier_data.csv' \n",
    "\n",
    "# This list defines all possible target columns in the file\n",
    "CLASSIFIER_TARGET_COLUMNS = [\n",
    "    'incident_type', 'incident_mechanism_1', 'incident_mechanism_2',\n",
    "    'incident_mechanism_3', 'eap_enacted_y_n_due_to_incident',\n",
    "    'fatalities_number', 'other_infrastructure_impacts', 'response',\n",
    "    'incident_report_produced'\n",
    "]\n",
    "\n",
    "# --- Main Processing Function (No changes needed here) ---\n",
    "def train_and_evaluate_model(X, y, target_name, summary_list):\n",
    "    \"\"\"\n",
    "    Trains a neural network and generates evaluation files.\n",
    "    If the target is binary, it appends a summary to the summary_list.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing target: {target_name} ---\")\n",
    "\n",
    "    # --- Pre-split Data Cleaning for Stratification ---\n",
    "    value_counts = y.value_counts()\n",
    "    single_sample_classes = value_counts[value_counts < 2].index\n",
    "\n",
    "    if not single_sample_classes.empty:\n",
    "        print(f\"Warning for target '{target_name}': Removing classes with only 1 sample: {list(single_sample_classes)}\")\n",
    "        original_count = len(y)\n",
    "        mask = ~y.isin(single_sample_classes)\n",
    "        X = X[mask].copy()\n",
    "        y = y[mask].copy()\n",
    "        print(f\"Removed {original_count - len(y)} rows.\")\n",
    "\n",
    "    if y.nunique() < 2:\n",
    "        print(f\"Skipping '{target_name}' because it has fewer than 2 valid classes after cleaning.\\n\")\n",
    "        return\n",
    "\n",
    "    # Identify features\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Convert target to codes\n",
    "    y_series = pd.Series(y).astype('category')\n",
    "    y_codes = y_series.cat.codes\n",
    "    class_names = y_series.cat.categories.tolist()\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_codes, test_size=0.2, random_state=42, stratify=y_codes)\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Handle class imbalance using SMOTE\n",
    "    min_class_samples = pd.Series(y_train).value_counts().min()\n",
    "    if y_series.nunique() > 1 and min_class_samples > 1:\n",
    "        k_neighbors = min(5, min_class_samples - 1)\n",
    "        print(f\"Applying SMOTE... Using k_neighbors={k_neighbors}.\")\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "    else:\n",
    "        print(f\"Skipping SMOTE for '{target_name}'.\")\n",
    "        X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "\n",
    "    # --- Build and Train Model ---\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train_resampled.shape[1],)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Training the model...\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model.fit(X_train_resampled, y_train_resampled, epochs=100, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # --- Save Model (Named by output) ---\n",
    "    model_filename = f'{target_name}.h5'\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    # --- Evaluate Model and Create Confusion Matrix ---\n",
    "    y_pred = np.argmax(model.predict(X_test_processed), axis=1)\n",
    "    all_class_labels = range(len(class_names))\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=all_class_labels)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # --- Save Confusion Matrix Plot ---\n",
    "    svg_filename = f'confusion_matrix_{target_name}.svg'\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {target_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(svg_filename, format='svg')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix plot saved as {svg_filename}\")\n",
    "\n",
    "    # --- Add results to the summary report if classification is binary ---\n",
    "    if len(class_names) == 2:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        summary_result = {\n",
    "            'Output Name': target_name,\n",
    "            'Actual 1, Predicted 1 (TP)': tp,\n",
    "            'Actual 0, Predicted 0 (TN)': tn,\n",
    "            'Actual 0, Predicted 1 (FP)': fp,\n",
    "            'Actual 1, Predicted 0 (FN)': fn\n",
    "        }\n",
    "        summary_list.append(summary_result)\n",
    "        print(f\"✅ Added binary classification results for '{target_name}' to summary.\")\n",
    "    else:\n",
    "        print(f\"ℹ️ Skipping summary for '{target_name}' (not a binary classification).\")\n",
    "\n",
    "    # --- Save detailed individual report ---\n",
    "    report_filename = f'report_{target_name}.xlsx'\n",
    "    results_df = X_test.copy()\n",
    "    results_df['actual_outcome'] = y.loc[X_test.index]\n",
    "    results_df['predicted_outcome'] = [class_names[i] for i in y_pred]\n",
    "    results_df.to_excel(report_filename, sheet_name='Test_Inputs_and_Predictions')\n",
    "    print(f\"Detailed report saved as {report_filename}\")\n",
    "    print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Main Execution (FIXED) ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The data file '{DATA_FILE}' was not found.\")\n",
    "        exit()\n",
    "\n",
    "    # Define the input columns dynamically by excluding all possible target columns\n",
    "    # This is more robust than maintaining a separate input column list.\n",
    "    input_cols = [col for col in df.columns if col not in CLASSIFIER_TARGET_COLUMNS]\n",
    "    X = df[input_cols]\n",
    "    \n",
    "    # --- Initialize a list to hold summary results for binary models ---\n",
    "    classification_summary_data = []\n",
    "\n",
    "    # Loop through each CLASSIFIER target variable and train a model\n",
    "    for target in CLASSIFIER_TARGET_COLUMNS:\n",
    "        # Check if the target column actually exists in the dataframe\n",
    "        if target not in df.columns:\n",
    "            print(f\"Warning: Target column '{target}' not found in the data file. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Drop rows where the current target is missing\n",
    "        temp_df = df.dropna(subset=[target])\n",
    "        X_filtered = temp_df[input_cols]\n",
    "        y = temp_df[target]\n",
    "\n",
    "        if y.nunique() < 2:\n",
    "            print(f\"Skipping '{target}' because it has less than 2 unique values.\")\n",
    "            continue\n",
    "            \n",
    "        train_and_evaluate_model(X_filtered, y, target, classification_summary_data)\n",
    "\n",
    "    # --- Save the consolidated binary classification summary to one Excel file ---\n",
    "    if classification_summary_data:\n",
    "        summary_df = pd.DataFrame(classification_summary_data)\n",
    "        summary_filename = 'binary_classification_summary.xlsx'\n",
    "        summary_df.to_excel(summary_filename, index=False)\n",
    "        print(f\"✅ All models trained. Binary summary saved to '{summary_filename}'.\")\n",
    "    else:\n",
    "        print(\"✅ All models trained. No binary classification tasks were run, so no summary file was created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28383e96-1e40-48a3-aed3-e342eb5ab530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing target: incident_type ---\n",
      "Warning for target 'incident_type': Removing classes with only 1 sample: [2]\n",
      "Removed 1 rows.\n",
      "Applying SMOTE... Using k_neighbors=5.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as incident_type.h5\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "✅ Performance metrics for 'incident_type' collected.\n",
      "Confusion matrix plot saved as confusion_matrix_incident_type.svg\n",
      "✅ Binary classification results for 'incident_type' added to summary.\n",
      "Detailed report saved as report_incident_type.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_1 ---\n",
      "Warning for target 'incident_mechanism_1': Removing classes with only 1 sample: [27, 28, 29]\n",
      "Removed 3 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as incident_mechanism_1.h5\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "✅ Performance metrics for 'incident_mechanism_1' collected.\n",
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_1.svg\n",
      "Detailed report saved as report_incident_mechanism_1.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_2 ---\n",
      "Warning for target 'incident_mechanism_2': Removing classes with only 1 sample: [13, 22, 23, 26, 27]\n",
      "Removed 5 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as incident_mechanism_2.h5\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "✅ Performance metrics for 'incident_mechanism_2' collected.\n",
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_2.svg\n",
      "Detailed report saved as report_incident_mechanism_2.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_3 ---\n",
      "Warning for target 'incident_mechanism_3': Removing classes with only 1 sample: [8, 10, 13, 15, 16, 17, 18]\n",
      "Removed 7 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as incident_mechanism_3.h5\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "✅ Performance metrics for 'incident_mechanism_3' collected.\n",
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_3.svg\n",
      "Detailed report saved as report_incident_mechanism_3.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: eap_enacted_y_n_due_to_incident ---\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as eap_enacted_y_n_due_to_incident.h5\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "✅ Performance metrics for 'eap_enacted_y_n_due_to_incident' collected.\n",
      "Confusion matrix plot saved as confusion_matrix_eap_enacted_y_n_due_to_incident.svg\n",
      "Detailed report saved as report_eap_enacted_y_n_due_to_incident.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: fatalities_number ---\n",
      "Warning for target 'fatalities_number': Removing classes with only 1 sample: [16, 22, 21, 20, 18, 12, 13, 11, 10, 9, 6, 5, 4, 24]\n",
      "Removed 14 rows.\n",
      "Skipping SMOTE for 'fatalities_number'.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as fatalities_number.h5\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "✅ Performance metrics for 'fatalities_number' collected.\n",
      "Confusion matrix plot saved as confusion_matrix_fatalities_number.svg\n",
      "Detailed report saved as report_fatalities_number.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: other_infrastructure_impacts ---\n",
      "Warning for target 'other_infrastructure_impacts': Removing classes with only 1 sample: [14, 13, 17, 4, 19, 20, 21]\n",
      "Removed 7 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as other_infrastructure_impacts.h5\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "✅ Performance metrics for 'other_infrastructure_impacts' collected.\n",
      "Confusion matrix plot saved as confusion_matrix_other_infrastructure_impacts.svg\n",
      "Detailed report saved as report_other_infrastructure_impacts.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: response ---\n",
      "Warning for target 'response': Removing classes with only 1 sample: [205, 135, 211, 138, 210, 140, 141, 142, 143, 144, 145, 146, 147, 148, 209, 150, 151, 152, 153, 154, 155, 156, 157, 136, 134, 159, 133, 213, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 212, 124, 125, 126, 127, 128, 129, 130, 131, 132, 158, 160, 204, 206, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 182, 181, 161, 180, 162, 208, 163, 111, 164, 165, 166, 167, 207, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 112, 107, 110, 109, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 28, 27, 26, 13, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 25, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 54, 56, 58, 96, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 98, 84, 99, 100, 101, 102, 103, 104, 105, 106, 1, 108, 85, 83, 59, 70, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 82, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 214]\n",
      "Removed 204 rows.\n",
      "Skipping SMOTE for 'response'.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as response.h5\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "✅ Performance metrics for 'response' collected.\n",
      "Confusion matrix plot saved as confusion_matrix_response.svg\n",
      "Detailed report saved as report_response.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_report_produced ---\n",
      "Warning for target 'incident_report_produced': Removing classes with only 1 sample: [7]\n",
      "Removed 1 rows.\n",
      "Applying SMOTE... Using k_neighbors=3.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as incident_report_produced.h5\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "✅ Performance metrics for 'incident_report_produced' collected.\n",
      "Confusion matrix plot saved as confusion_matrix_incident_report_produced.svg\n",
      "Detailed report saved as report_incident_report_produced.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "✅ Binary summary saved to 'binary_classification_summary.xlsx'.\n",
      "✅ Performance metrics for all models saved to 'model_performance_metrics.xlsx'.\n",
      "\n",
      "All tasks complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "import os\n",
    "\n",
    "# Try to import imblearn, provide install instructions if it fails\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except ImportError:\n",
    "    print(\"Error: The 'imbalanced-learn' library is required but not installed.\")\n",
    "    print(\"Please install it by running the following command in your terminal:\")\n",
    "    print(\"pip install imbalanced-learn\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = '../classifier_data.csv' \n",
    "\n",
    "CLASSIFIER_TARGET_COLUMNS = [\n",
    "    'incident_type', 'incident_mechanism_1', 'incident_mechanism_2',\n",
    "    'incident_mechanism_3', 'eap_enacted_y_n_due_to_incident',\n",
    "    'fatalities_number', 'other_infrastructure_impacts', 'response',\n",
    "    'incident_report_produced'\n",
    "]\n",
    "\n",
    "# --- Main Processing Function ---\n",
    "def train_and_evaluate_model(X, y, target_name, summary_list, metrics_list):\n",
    "    \"\"\"\n",
    "    Trains a neural network, generates evaluation files, and collects performance metrics.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing target: {target_name} ---\")\n",
    "\n",
    "    # --- Pre-split Data Cleaning ---\n",
    "    value_counts = y.value_counts()\n",
    "    single_sample_classes = value_counts[value_counts < 2].index\n",
    "\n",
    "    if not single_sample_classes.empty:\n",
    "        print(f\"Warning for target '{target_name}': Removing classes with only 1 sample: {list(single_sample_classes)}\")\n",
    "        mask = ~y.isin(single_sample_classes)\n",
    "        X = X[mask].copy()\n",
    "        y = y[mask].copy()\n",
    "        print(f\"Removed {len(single_sample_classes)} rows.\")\n",
    "\n",
    "    if y.nunique() < 2:\n",
    "        print(f\"Skipping '{target_name}' because it has fewer than 2 valid classes.\\n\")\n",
    "        return\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    y_series = pd.Series(y).astype('category')\n",
    "    y_codes = y_series.cat.codes\n",
    "    class_names = y_series.cat.categories.tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_codes, test_size=0.2, random_state=42, stratify=y_codes)\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # --- SMOTE for Imbalanced Data ---\n",
    "    min_class_samples = pd.Series(y_train).value_counts().min()\n",
    "    if y_series.nunique() > 1 and min_class_samples > 1:\n",
    "        k_neighbors = min(5, min_class_samples - 1)\n",
    "        print(f\"Applying SMOTE... Using k_neighbors={k_neighbors}.\")\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "    else:\n",
    "        print(f\"Skipping SMOTE for '{target_name}'.\")\n",
    "        X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "\n",
    "    # --- Build and Train Model ---\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train_resampled.shape[1],)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Training the model...\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model.fit(X_train_resampled, y_train_resampled, epochs=100, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    model_filename = f'{target_name}.h5'\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    y_pred = np.argmax(model.predict(X_test_processed), axis=1)\n",
    "\n",
    "    # --- NEW: Calculate and Collect Performance Metrics ---\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Use 'weighted' average for multi-class precision, recall, and F1\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    metrics_result = {\n",
    "        'Model Output': target_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision (Weighted)': precision,\n",
    "        'Recall (Weighted)': recall,\n",
    "        'F1-Score (Weighted)': f1\n",
    "    }\n",
    "    metrics_list.append(metrics_result)\n",
    "    print(f\"✅ Performance metrics for '{target_name}' collected.\")\n",
    "\n",
    "    # --- Generate Other Reports ---\n",
    "    all_class_labels = range(len(class_names))\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=all_class_labels)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    \n",
    "    svg_filename = f'confusion_matrix_{target_name}.svg'\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {target_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(svg_filename, format='svg')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix plot saved as {svg_filename}\")\n",
    "\n",
    "    # Add results to the binary summary report\n",
    "    if len(class_names) == 2:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        summary_result = {\n",
    "            'Output Name': target_name,\n",
    "            'Actual 1, Predicted 1 (TP)': tp,\n",
    "            'Actual 0, Predicted 0 (TN)': tn,\n",
    "            'Actual 0, Predicted 1 (FP)': fp,\n",
    "            'Actual 1, Predicted 0 (FN)': fn\n",
    "        }\n",
    "        summary_list.append(summary_result)\n",
    "        print(f\"✅ Binary classification results for '{target_name}' added to summary.\")\n",
    "    \n",
    "    # Save detailed individual report\n",
    "    report_filename = f'report_{target_name}.xlsx'\n",
    "    results_df = X_test.copy()\n",
    "    results_df['actual_outcome'] = y.loc[X_test.index]\n",
    "    results_df['predicted_outcome'] = [class_names[i] for i in y_pred]\n",
    "    results_df.to_excel(report_filename, sheet_name='Test_Inputs_and_Predictions')\n",
    "    print(f\"Detailed report saved as {report_filename}\")\n",
    "    print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The data file '{DATA_FILE}' was not found.\")\n",
    "        exit()\n",
    "\n",
    "    input_cols = [col for col in df.columns if col not in CLASSIFIER_TARGET_COLUMNS]\n",
    "    X = df[input_cols]\n",
    "    \n",
    "    # Initialize lists to hold summary data from all models\n",
    "    binary_summary_data = []\n",
    "    model_metrics_data = [] # NEW list for performance metrics\n",
    "\n",
    "    # Loop through each CLASSIFIER target variable\n",
    "    for target in CLASSIFIER_TARGET_COLUMNS:\n",
    "        if target not in df.columns:\n",
    "            print(f\"Warning: Target column '{target}' not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        temp_df = df.dropna(subset=[target])\n",
    "        X_filtered = temp_df[input_cols]\n",
    "        y = temp_df[target]\n",
    "\n",
    "        if y.nunique() < 2:\n",
    "            print(f\"Skipping '{target}' because it has less than 2 unique values.\")\n",
    "            continue\n",
    "            \n",
    "        train_and_evaluate_model(X_filtered, y, target, binary_summary_data, model_metrics_data)\n",
    "\n",
    "    # --- Save Consolidated Reports ---\n",
    "    if binary_summary_data:\n",
    "        summary_df = pd.DataFrame(binary_summary_data)\n",
    "        summary_filename = 'binary_classification_summary.xlsx'\n",
    "        summary_df.to_excel(summary_filename, index=False)\n",
    "        print(f\"✅ Binary summary saved to '{summary_filename}'.\")\n",
    "    else:\n",
    "        print(\"ℹ️ No binary classification tasks were run, so no binary summary file was created.\")\n",
    "\n",
    "    # NEW: Save the consolidated performance metrics to one Excel file\n",
    "    if model_metrics_data:\n",
    "        metrics_df = pd.DataFrame(model_metrics_data)\n",
    "        metrics_filename = 'model_performance_metrics.xlsx'\n",
    "        metrics_df.to_excel(metrics_filename, index=False)\n",
    "        print(f\"✅ Performance metrics for all models saved to '{metrics_filename}'.\")\n",
    "\n",
    "    print(\"\\nAll tasks complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
