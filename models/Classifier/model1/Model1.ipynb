{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17c907-29a0-4605-95fb-14a1f0ff23c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing target: incident_type ---\n",
      "Warning for target 'incident_type': Removing classes with only 1 sample: [2]\n",
      "Removed 1 rows.\n",
      "Applying SMOTE... Using k_neighbors=5.\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Training the model...\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as incident_type.h5\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "Confusion matrix plot saved as confusion_matrix_incident_type.svg\n",
      "✅ Added binary classification results for 'incident_type' to summary.\n",
      "Detailed report saved as report_incident_type.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_1 ---\n",
      "Warning for target 'incident_mechanism_1': Removing classes with only 1 sample: [27, 28, 29]\n",
      "Removed 3 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "import os\n",
    "\n",
    "# Try to import imblearn, provide install instructions if it fails\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except ImportError:\n",
    "    print(\"Error: The 'imbalanced-learn' library is required but not installed.\")\n",
    "    print(\"Please install it by running the following command in your terminal:\")\n",
    "    print(\"pip install imbalanced-learn\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# Using the pre-processed classifier data file\n",
    "DATA_FILE = '../classifier_data.csv' \n",
    "\n",
    "# This list defines all possible target columns in the file\n",
    "CLASSIFIER_TARGET_COLUMNS = [\n",
    "    'incident_type', 'incident_mechanism_1', 'incident_mechanism_2',\n",
    "    'incident_mechanism_3', 'eap_enacted_y_n_due_to_incident',\n",
    "    'fatalities_number', 'other_infrastructure_impacts', 'response',\n",
    "    'incident_report_produced'\n",
    "]\n",
    "\n",
    "# --- Main Processing Function (No changes needed here) ---\n",
    "def train_and_evaluate_model(X, y, target_name, summary_list):\n",
    "    \"\"\"\n",
    "    Trains a neural network and generates evaluation files.\n",
    "    If the target is binary, it appends a summary to the summary_list.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing target: {target_name} ---\")\n",
    "\n",
    "    # --- Pre-split Data Cleaning for Stratification ---\n",
    "    value_counts = y.value_counts()\n",
    "    single_sample_classes = value_counts[value_counts < 2].index\n",
    "\n",
    "    if not single_sample_classes.empty:\n",
    "        print(f\"Warning for target '{target_name}': Removing classes with only 1 sample: {list(single_sample_classes)}\")\n",
    "        original_count = len(y)\n",
    "        mask = ~y.isin(single_sample_classes)\n",
    "        X = X[mask].copy()\n",
    "        y = y[mask].copy()\n",
    "        print(f\"Removed {original_count - len(y)} rows.\")\n",
    "\n",
    "    if y.nunique() < 2:\n",
    "        print(f\"Skipping '{target_name}' because it has fewer than 2 valid classes after cleaning.\\n\")\n",
    "        return\n",
    "\n",
    "    # Identify features\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Convert target to codes\n",
    "    y_series = pd.Series(y).astype('category')\n",
    "    y_codes = y_series.cat.codes\n",
    "    class_names = y_series.cat.categories.tolist()\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_codes, test_size=0.2, random_state=42, stratify=y_codes)\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Handle class imbalance using SMOTE\n",
    "    min_class_samples = pd.Series(y_train).value_counts().min()\n",
    "    if y_series.nunique() > 1 and min_class_samples > 1:\n",
    "        k_neighbors = min(5, min_class_samples - 1)\n",
    "        print(f\"Applying SMOTE... Using k_neighbors={k_neighbors}.\")\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "    else:\n",
    "        print(f\"Skipping SMOTE for '{target_name}'.\")\n",
    "        X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "\n",
    "    # --- Build and Train Model ---\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train_resampled.shape[1],)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Training the model...\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model.fit(X_train_resampled, y_train_resampled, epochs=100, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # --- Save Model (Named by output) ---\n",
    "    model_filename = f'{target_name}.h5'\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    # --- Evaluate Model and Create Confusion Matrix ---\n",
    "    y_pred = np.argmax(model.predict(X_test_processed), axis=1)\n",
    "    all_class_labels = range(len(class_names))\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=all_class_labels)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # --- Save Confusion Matrix Plot ---\n",
    "    svg_filename = f'confusion_matrix_{target_name}.svg'\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {target_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(svg_filename, format='svg')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix plot saved as {svg_filename}\")\n",
    "\n",
    "    # --- Add results to the summary report if classification is binary ---\n",
    "    if len(class_names) == 2:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        summary_result = {\n",
    "            'Output Name': target_name,\n",
    "            'Actual 1, Predicted 1 (TP)': tp,\n",
    "            'Actual 0, Predicted 0 (TN)': tn,\n",
    "            'Actual 0, Predicted 1 (FP)': fp,\n",
    "            'Actual 1, Predicted 0 (FN)': fn\n",
    "        }\n",
    "        summary_list.append(summary_result)\n",
    "        print(f\"✅ Added binary classification results for '{target_name}' to summary.\")\n",
    "    else:\n",
    "        print(f\"ℹ️ Skipping summary for '{target_name}' (not a binary classification).\")\n",
    "\n",
    "    # --- Save detailed individual report ---\n",
    "    report_filename = f'report_{target_name}.xlsx'\n",
    "    results_df = X_test.copy()\n",
    "    results_df['actual_outcome'] = y.loc[X_test.index]\n",
    "    results_df['predicted_outcome'] = [class_names[i] for i in y_pred]\n",
    "    results_df.to_excel(report_filename, sheet_name='Test_Inputs_and_Predictions')\n",
    "    print(f\"Detailed report saved as {report_filename}\")\n",
    "    print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Main Execution (FIXED) ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The data file '{DATA_FILE}' was not found.\")\n",
    "        exit()\n",
    "\n",
    "    # Define the input columns dynamically by excluding all possible target columns\n",
    "    # This is more robust than maintaining a separate input column list.\n",
    "    input_cols = [col for col in df.columns if col not in CLASSIFIER_TARGET_COLUMNS]\n",
    "    X = df[input_cols]\n",
    "    \n",
    "    # --- Initialize a list to hold summary results for binary models ---\n",
    "    classification_summary_data = []\n",
    "\n",
    "    # Loop through each CLASSIFIER target variable and train a model\n",
    "    for target in CLASSIFIER_TARGET_COLUMNS:\n",
    "        # Check if the target column actually exists in the dataframe\n",
    "        if target not in df.columns:\n",
    "            print(f\"Warning: Target column '{target}' not found in the data file. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Drop rows where the current target is missing\n",
    "        temp_df = df.dropna(subset=[target])\n",
    "        X_filtered = temp_df[input_cols]\n",
    "        y = temp_df[target]\n",
    "\n",
    "        if y.nunique() < 2:\n",
    "            print(f\"Skipping '{target}' because it has less than 2 unique values.\")\n",
    "            continue\n",
    "            \n",
    "        train_and_evaluate_model(X_filtered, y, target, classification_summary_data)\n",
    "\n",
    "    # --- Save the consolidated binary classification summary to one Excel file ---\n",
    "    if classification_summary_data:\n",
    "        summary_df = pd.DataFrame(classification_summary_data)\n",
    "        summary_filename = 'binary_classification_summary.xlsx'\n",
    "        summary_df.to_excel(summary_filename, index=False)\n",
    "        print(f\"✅ All models trained. Binary summary saved to '{summary_filename}'.\")\n",
    "    else:\n",
    "        print(\"✅ All models trained. No binary classification tasks were run, so no summary file was created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
