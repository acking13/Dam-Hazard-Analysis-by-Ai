{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892034bb-7cc2-4e53-9d82-86e83274c20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing target: incident_type ---\n",
      "Warning for target 'incident_type': The following classes have only 1 sample and will be removed: [2]\n",
      "Removed 1 rows.\n",
      "Applying SMOTE for class imbalance...\n",
      "Smallest class in training set has 389 samples. Adjusting SMOTE k_neighbors to 5.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as model_incident_type.h5\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "Confusion matrix saved as confusion_matrix_incident_type.svg\n",
      "Excel report saved as report_incident_type.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_1 ---\n",
      "Warning for target 'incident_mechanism_1': The following classes have only 1 sample and will be removed: [27, 28, 29]\n",
      "Removed 3 rows.\n",
      "Applying SMOTE for class imbalance...\n",
      "Smallest class in training set has 2 samples. Adjusting SMOTE k_neighbors to 1.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as model_incident_mechanism_1.h5\n",
      "9/9 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (26, 26), indices imply (27, 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 210\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     y \u001b[38;5;241m=\u001b[39m df[target]\n\u001b[1;32m--> 210\u001b[0m     \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll models have been trained and evaluated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 154\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(X, y, target_name)\u001b[0m\n\u001b[0;32m    151\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred_proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    153\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_pred)\n\u001b[1;32m--> 154\u001b[0m cm_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# --- Save Confusion Matrix as SVG ---\u001b[39;00m\n\u001b[0;32m    157\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:758\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    747\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    748\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    749\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    755\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    756\u001b[0m         )\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 758\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:337\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    333\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    334\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    335\u001b[0m )\n\u001b[1;32m--> 337\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:408\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    406\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    407\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 408\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (26, 26), indices imply (27, 27)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except ImportError:\n",
    "    print(\"Error: The 'imbalanced-learn' library is required but not installed.\")\n",
    "    print(\"Please install it by running the following command in your terminal:\")\n",
    "    print(\"pip install imbalanced-learn\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = '../encoded_dam_data.csv'\n",
    "INPUT_COLUMNS = [\n",
    "    'state', 'downstream_hazard_potential', 'incident_date', 'incident_time',\n",
    "    'incident_driver', 'owner_type', 'dam_type', 'primary_purpose_s', 'eap',\n",
    "    'dam_height', 'max_storage_ac_ft', 'surface_area_acres', 'year_completed',\n",
    "    'latitude', 'longitude', 'year_modified'\n",
    "]\n",
    "OUTPUT_COLUMNS = [\n",
    "    'incident_type', 'incident_mechanism_1', 'incident_mechanism_2',\n",
    "    'incident_mechanism_3', 'eap_enacted_y_n_due_to_incident',\n",
    "    'fatalities_number', 'number_of_people_evacuated',\n",
    "    'number_of_habitable_structures_evacuated',\n",
    "    'number_of_habitable_structures_flooded', 'other_infrastructure_impacts',\n",
    "    'response', 'volume_released_at_failure_ac_ft', 'incident_duration',\n",
    "    'incident_report_produced'\n",
    "]\n",
    "\n",
    "# --- Main Processing Function ---\n",
    "def train_and_evaluate_model(X, y, target_name):\n",
    "    \"\"\"\n",
    "    Trains a neural network model for a given target variable, saves the model,\n",
    "    and generates evaluation files (confusion matrix SVG and Excel report).\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing target: {target_name} ---\")\n",
    "\n",
    "    # --- Pre-split Data Cleaning for Stratification ---\n",
    "    # Stratified split requires at least 2 members per class.\n",
    "    # We identify classes with only one sample and remove them.\n",
    "    value_counts = y.value_counts()\n",
    "    single_sample_classes = value_counts[value_counts < 2].index\n",
    "\n",
    "    if not single_sample_classes.empty:\n",
    "        print(f\"Warning for target '{target_name}': The following classes have only 1 sample and will be removed: {list(single_sample_classes)}\")\n",
    "        # Keep only the data that does not belong to the single-sample classes\n",
    "        original_count = len(y)\n",
    "        mask = ~y.isin(single_sample_classes)\n",
    "        X = X[mask].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "        y = y[mask].copy()\n",
    "        print(f\"Removed {original_count - len(y)} rows.\")\n",
    "\n",
    "    # If after cleaning, we have less than 2 classes, we can't classify.\n",
    "    if y.nunique() < 2:\n",
    "        print(f\"Skipping '{target_name}' because it has less than 2 valid classes after cleaning.\\n\")\n",
    "        return # Exit the function for this target\n",
    "\n",
    "\n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "    # Create preprocessing pipelines for numerical and categorical features\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Create a preprocessor object using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "    # Convert target variable to categorical and then to integer codes\n",
    "    y_series = pd.Series(y).astype('category')\n",
    "    y_codes = y_series.cat.codes\n",
    "    class_names = y_series.cat.categories.tolist()\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_codes, test_size=0.2, random_state=42, stratify=y_codes)\n",
    "\n",
    "    # Apply preprocessing\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Handle class imbalance using SMOTE\n",
    "    # Note: SMOTE is applied only to the training data\n",
    "    if len(class_names) > 1:\n",
    "        # SMOTE requires the number of neighbors to be less than the number of samples in the smallest class.\n",
    "        # We check the training data for the smallest class count.\n",
    "        min_class_samples = pd.Series(y_train).value_counts().min()\n",
    "\n",
    "        # We can only use SMOTE if the smallest class has at least 2 samples.\n",
    "        if min_class_samples > 1:\n",
    "            # We must set k_neighbors to be less than the number of samples in the smallest class.\n",
    "            # We'll use the default of 5 if possible, otherwise we reduce it.\n",
    "            k_neighbors = min(5, min_class_samples - 1)\n",
    "\n",
    "            print(\"Applying SMOTE for class imbalance...\")\n",
    "            print(f\"Smallest class in training set has {min_class_samples} samples. Adjusting SMOTE k_neighbors to {k_neighbors}.\")\n",
    "            smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "        else:\n",
    "            print(f\"Skipping SMOTE for '{target_name}': the smallest class in the training set has only {min_class_samples} sample(s).\")\n",
    "            X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "    else:\n",
    "        X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "\n",
    "\n",
    "    # --- Build the Neural Network Model ---\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train_resampled.shape[1],)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # --- Train the Model ---\n",
    "    print(\"Training the model...\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_train_resampled, y_train_resampled,\n",
    "                        epochs=100,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=0) # Set to 1 to see training progress\n",
    "\n",
    "    # --- Save the Trained Model ---\n",
    "    model_filename = f'model_{target_name}.h5'\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    # --- Evaluate the Model and Generate Confusion Matrix ---\n",
    "    y_pred_proba = model.predict(X_test_processed)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # --- Save Confusion Matrix as SVG ---\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {target_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    svg_filename = f'confusion_matrix_{target_name}.svg'\n",
    "    plt.savefig(svg_filename, format='svg')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved as {svg_filename}\")\n",
    "\n",
    "    # --- Save Results to Excel ---\n",
    "    excel_filename = f'report_{target_name}.xlsx'\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "        cm_df.to_excel(writer, sheet_name='Confusion Matrix')\n",
    "        X_test.assign(actual=y_series.iloc[y_test.index].values, predicted=y_series.cat.categories[y_pred]).to_excel(writer, sheet_name='Test Inputs and Predictions')\n",
    "    print(f\"Excel report saved as {excel_filename}\")\n",
    "    print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The data file '{DATA_FILE}' was not found.\")\n",
    "        print(\"Please make sure the CSV file is in the same directory as the script.\")\n",
    "        exit()\n",
    "\n",
    "    # Drop rows where any of the target columns are missing\n",
    "    df.dropna(subset=OUTPUT_COLUMNS, inplace=True)\n",
    "\n",
    "    # Convert date/time columns to numerical features\n",
    "    for col in ['incident_date', 'incident_time']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            df[f'{col}_year'] = df[col].dt.year\n",
    "            df[f'{col}_month'] = df[col].dt.month\n",
    "            df[f'{col}_day'] = df[col].dt.day\n",
    "            if col == 'incident_time':\n",
    "                 df[f'{col}_hour'] = df[col].dt.hour\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "            # Add new generated columns to input columns list\n",
    "            INPUT_COLUMNS.remove(col)\n",
    "            INPUT_COLUMNS.extend([c for c in df.columns if c.startswith(col)])\n",
    "\n",
    "\n",
    "    X = df[INPUT_COLUMNS]\n",
    "\n",
    "    for target in OUTPUT_COLUMNS:\n",
    "        if df[target].nunique() < 2:\n",
    "            print(f\"Skipping '{target}' because it has less than 2 unique values.\")\n",
    "            continue\n",
    "        y = df[target]\n",
    "        train_and_evaluate_model(X, y, target)\n",
    "\n",
    "    print(\"All models have been trained and evaluated.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96b8125-0feb-4b6e-8538-5ac1d2e23d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training loop. All confusion matrices will be saved in 'all_confusion_matrices.xlsx'.\n",
      "\n",
      "--- Processing target: incident_type ---\n",
      "Warning for target 'incident_type': Removing classes with only 1 sample: [2]\n",
      "Removed 1 rows.\n",
      "Applying SMOTE... Smallest class in training set has 389 samples. Using k_neighbors=5.\n",
      "Training the model...\n",
      "Model saved as model_incident_type.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_type.svg\n",
      "Confusion matrix data added to Excel sheet: 'CM_incident_type'\n",
      "Detailed report saved as report_incident_type.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_1 ---\n",
      "Warning for target 'incident_mechanism_1': Removing classes with only 1 sample: [27, 28, 29]\n",
      "Removed 3 rows.\n",
      "Applying SMOTE... Smallest class in training set has 2 samples. Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as model_incident_mechanism_1.h5\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_1.svg\n",
      "Confusion matrix data added to Excel sheet: 'CM_incident_mechanism_1'\n",
      "Detailed report saved as report_incident_mechanism_1.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_2 ---\n",
      "Warning for target 'incident_mechanism_2': Removing classes with only 1 sample: [13, 22, 23, 26, 27]\n",
      "Removed 5 rows.\n",
      "Applying SMOTE... Smallest class in training set has 2 samples. Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as model_incident_mechanism_2.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_2.svg\n",
      "Confusion matrix data added to Excel sheet: 'CM_incident_mechanism_2'\n",
      "Detailed report saved as report_incident_mechanism_2.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_3 ---\n",
      "Warning for target 'incident_mechanism_3': Removing classes with only 1 sample: [8, 10, 13, 15, 16, 17, 18]\n",
      "Removed 7 rows.\n",
      "Applying SMOTE... Smallest class in training set has 2 samples. Using k_neighbors=1.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "import os\n",
    "\n",
    "# Try to import imblearn, provide install instructions if it fails\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except ImportError:\n",
    "    print(\"Error: The 'imbalanced-learn' library is required but not installed.\")\n",
    "    print(\"Please install it by running the following command in your terminal:\")\n",
    "    print(\"pip install imbalanced-learn\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = '../encoded_dam_data.csv' # Make sure this path is correct\n",
    "INPUT_COLUMNS = [\n",
    "    'state', 'downstream_hazard_potential', 'incident_date', 'incident_time',\n",
    "    'incident_driver', 'owner_type', 'dam_type', 'primary_purpose_s', 'eap',\n",
    "    'dam_height', 'max_storage_ac_ft', 'surface_area_acres', 'year_completed',\n",
    "    'latitude', 'longitude', 'year_modified'\n",
    "]\n",
    "OUTPUT_COLUMNS = [\n",
    "    'incident_type', 'incident_mechanism_1', 'incident_mechanism_2',\n",
    "    'incident_mechanism_3', 'eap_enacted_y_n_due_to_incident',\n",
    "    'fatalities_number', 'number_of_people_evacuated',\n",
    "    'number_of_habitable_structures_evacuated',\n",
    "    'number_of_habitable_structures_flooded', 'other_infrastructure_impacts',\n",
    "    'response', 'volume_released_at_failure_ac_ft', 'incident_duration',\n",
    "    'incident_report_produced'\n",
    "]\n",
    "\n",
    "# --- Main Processing Function ---\n",
    "def train_and_evaluate_model(X, y, target_name, excel_writer):\n",
    "    \"\"\"\n",
    "    Trains a neural network, saves the model, generates evaluation files,\n",
    "    and writes the confusion matrix to a shared Excel writer object.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing target: {target_name} ---\")\n",
    "\n",
    "    # --- Pre-split Data Cleaning for Stratification ---\n",
    "    # Stratified split requires at least 2 members per class.\n",
    "    value_counts = y.value_counts()\n",
    "    single_sample_classes = value_counts[value_counts < 2].index\n",
    "\n",
    "    if not single_sample_classes.empty:\n",
    "        print(f\"Warning for target '{target_name}': Removing classes with only 1 sample: {list(single_sample_classes)}\")\n",
    "        original_count = len(y)\n",
    "        mask = ~y.isin(single_sample_classes)\n",
    "        X = X[mask].copy()\n",
    "        y = y[mask].copy()\n",
    "        print(f\"Removed {original_count - len(y)} rows.\")\n",
    "\n",
    "    if y.nunique() < 2:\n",
    "        print(f\"Skipping '{target_name}' because it has fewer than 2 valid classes after cleaning.\\n\")\n",
    "        return\n",
    "\n",
    "    # Identify categorical and numerical features from the provided X\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough' # Keep other columns if any\n",
    "    )\n",
    "\n",
    "    # Convert target variable to categorical codes for the model\n",
    "    y_series = pd.Series(y).astype('category')\n",
    "    y_codes = y_series.cat.codes\n",
    "    class_names = y_series.cat.categories.tolist()\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_codes, test_size=0.2, random_state=42, stratify=y_codes)\n",
    "\n",
    "    # Apply preprocessing\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Handle class imbalance using SMOTE on the training data only\n",
    "    min_class_samples = pd.Series(y_train).value_counts().min()\n",
    "    if y_series.nunique() > 1 and min_class_samples > 1:\n",
    "        k_neighbors = min(5, min_class_samples - 1)\n",
    "        print(f\"Applying SMOTE... Smallest class in training set has {min_class_samples} samples. Using k_neighbors={k_neighbors}.\")\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "    else:\n",
    "        print(f\"Skipping SMOTE for '{target_name}': smallest class in training set has {min_class_samples} sample(s).\")\n",
    "        X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "\n",
    "    # --- Build the Neural Network Model ---\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train_resampled.shape[1],)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # --- Train the Model ---\n",
    "    print(\"Training the model...\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model.fit(X_train_resampled, y_train_resampled,\n",
    "              epochs=100,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[early_stopping],\n",
    "              verbose=0) # Set to 1 to see training progress\n",
    "\n",
    "    # --- Save the Trained Model ---\n",
    "    model_filename = f'model_{target_name}.h5'\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    # --- Evaluate the Model ---\n",
    "    y_pred_proba = model.predict(X_test_processed)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "    # Define the full range of possible class labels\n",
    "    all_class_labels = range(len(class_names))\n",
    "    # Create the confusion matrix using all possible labels to ensure correct shape\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=all_class_labels)\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # --- Save Confusion Matrix as SVG ---\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {target_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    svg_filename = f'confusion_matrix_{target_name}.svg'\n",
    "    plt.savefig(svg_filename, format='svg')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix plot saved as {svg_filename}\")\n",
    "\n",
    "    # --- Write Confusion Matrix to the shared Excel file ---\n",
    "    # The sheet name is sanitized to be valid\n",
    "    safe_sheet_name = f'CM_{target_name[:25]}'\n",
    "    cm_df.to_excel(excel_writer, sheet_name=safe_sheet_name)\n",
    "    print(f\"Confusion matrix data added to Excel sheet: '{safe_sheet_name}'\")\n",
    "\n",
    "    # --- Save individual detailed report to a separate Excel file ---\n",
    "    report_filename = f'report_{target_name}.xlsx'\n",
    "    results_df = X_test.copy()\n",
    "    results_df['actual_outcome'] = y.loc[X_test.index]\n",
    "    results_df['predicted_outcome'] = [class_names[i] for i in y_pred]\n",
    "    results_df.to_excel(report_filename, sheet_name='Test_Inputs_and_Predictions')\n",
    "    print(f\"Detailed report saved as {report_filename}\")\n",
    "    print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The data file '{DATA_FILE}' was not found.\")\n",
    "        print(\"Please ensure the file path is correct.\")\n",
    "        exit()\n",
    "\n",
    "    # Drop rows where any of the target columns are missing\n",
    "    df.dropna(subset=OUTPUT_COLUMNS, inplace=True)\n",
    "\n",
    "    # Create a copy of input columns to modify\n",
    "    processed_input_cols = INPUT_COLUMNS.copy()\n",
    "\n",
    "    # Convert date/time columns to numerical features\n",
    "    for col in ['incident_date', 'incident_time']:\n",
    "        if col in df.columns and col in processed_input_cols:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            df[f'{col}_year'] = df[col].dt.year\n",
    "            df[f'{col}_month'] = df[col].dt.month\n",
    "            df[f'{col}_day'] = df[col].dt.day\n",
    "            new_cols = [f'{col}_year', f'{col}_month', f'{col}_day']\n",
    "            if col == 'incident_time':\n",
    "                df[f'{col}_hour'] = df[col].dt.hour\n",
    "                new_cols.append(f'{col}_hour')\n",
    "            \n",
    "            # Update the list of columns to be used as inputs\n",
    "            processed_input_cols.remove(col)\n",
    "            processed_input_cols.extend(new_cols)\n",
    "    \n",
    "    # Drop rows with NaT in date columns after coercion\n",
    "    df.dropna(subset=processed_input_cols, inplace=True)\n",
    "    \n",
    "    # Define X *after* processing columns\n",
    "    # Ensure all columns exist in the dataframe before selection\n",
    "    final_input_cols = [col for col in processed_input_cols if col in df.columns]\n",
    "    X = df[final_input_cols]\n",
    "\n",
    "    # Create a single Excel writer for all confusion matrices\n",
    "    excel_cm_filename = 'all_confusion_matrices.xlsx'\n",
    "    with pd.ExcelWriter(excel_cm_filename, engine='openpyxl') as writer:\n",
    "        print(f\"Starting model training loop. All confusion matrices will be saved in '{excel_cm_filename}'.\\n\")\n",
    "        # Loop through each target variable and train a model\n",
    "        for target in OUTPUT_COLUMNS:\n",
    "            if df[target].nunique() < 2:\n",
    "                print(f\"Skipping '{target}' because it has less than 2 unique values.\")\n",
    "                continue\n",
    "            y = df[target]\n",
    "            # Pass the writer object to the function\n",
    "            train_and_evaluate_model(X, y, target, writer)\n",
    "\n",
    "    print(\"✅ All models have been trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33161f5c-258c-4a4b-a6f0-6a2d270e54ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e4dab-2d6b-4ecc-8912-02291cb76b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fd516-e5b8-464e-9cf7-120d6351fbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
