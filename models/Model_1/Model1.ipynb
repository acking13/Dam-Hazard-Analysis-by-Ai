{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892034bb-7cc2-4e53-9d82-86e83274c20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96b8125-0feb-4b6e-8538-5ac1d2e23d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training loop. All confusion matrices will be saved in 'all_confusion_matrices.xlsx'.\n",
      "\n",
      "--- Processing target: incident_type ---\n",
      "Warning for target 'incident_type': Removing classes with only 1 sample: [2]\n",
      "Removed 1 rows.\n",
      "Applying SMOTE... Smallest class in training set has 389 samples. Using k_neighbors=5.\n",
      "Training the model...\n",
      "Model saved as model_incident_type.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_type.svg\n",
      "Confusion matrix data added to Excel sheet: 'CM_incident_type'\n",
      "Detailed report saved as report_incident_type.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_1 ---\n",
      "Warning for target 'incident_mechanism_1': Removing classes with only 1 sample: [27, 28, 29]\n",
      "Removed 3 rows.\n",
      "Applying SMOTE... Smallest class in training set has 2 samples. Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as model_incident_mechanism_1.h5\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_1.svg\n",
      "Confusion matrix data added to Excel sheet: 'CM_incident_mechanism_1'\n",
      "Detailed report saved as report_incident_mechanism_1.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_2 ---\n",
      "Warning for target 'incident_mechanism_2': Removing classes with only 1 sample: [13, 22, 23, 26, 27]\n",
      "Removed 5 rows.\n",
      "Applying SMOTE... Smallest class in training set has 2 samples. Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as model_incident_mechanism_2.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_2.svg\n",
      "Confusion matrix data added to Excel sheet: 'CM_incident_mechanism_2'\n",
      "Detailed report saved as report_incident_mechanism_2.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_3 ---\n",
      "Warning for target 'incident_mechanism_3': Removing classes with only 1 sample: [8, 10, 13, 15, 16, 17, 18]\n",
      "Removed 7 rows.\n",
      "Applying SMOTE... Smallest class in training set has 2 samples. Using k_neighbors=1.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "import os\n",
    "\n",
    "# Try to import imblearn, provide install instructions if it fails\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except ImportError:\n",
    "    print(\"Error: The 'imbalanced-learn' library is required but not installed.\")\n",
    "    print(\"Please install it by running the following command in your terminal:\")\n",
    "    print(\"pip install imbalanced-learn\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = '../encoded_dam_data.csv' # Make sure this path is correct\n",
    "INPUT_COLUMNS = [\n",
    "    'state', 'downstream_hazard_potential', 'incident_date', 'incident_time',\n",
    "    'incident_driver', 'owner_type', 'dam_type', 'primary_purpose_s', 'eap',\n",
    "    'dam_height', 'max_storage_ac_ft', 'surface_area_acres', 'year_completed',\n",
    "    'latitude', 'longitude', 'year_modified'\n",
    "]\n",
    "OUTPUT_COLUMNS = [\n",
    "    'incident_type', 'incident_mechanism_1', 'incident_mechanism_2',\n",
    "    'incident_mechanism_3', 'eap_enacted_y_n_due_to_incident',\n",
    "    'fatalities_number', 'number_of_people_evacuated',\n",
    "    'number_of_habitable_structures_evacuated',\n",
    "    'number_of_habitable_structures_flooded', 'other_infrastructure_impacts',\n",
    "    'response', 'volume_released_at_failure_ac_ft', 'incident_duration',\n",
    "    'incident_report_produced'\n",
    "]\n",
    "\n",
    "# --- Main Processing Function ---\n",
    "def train_and_evaluate_model(X, y, target_name, excel_writer):\n",
    "    \"\"\"\n",
    "    Trains a neural network, saves the model, generates evaluation files,\n",
    "    and writes the confusion matrix to a shared Excel writer object.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing target: {target_name} ---\")\n",
    "\n",
    "    # --- Pre-split Data Cleaning for Stratification ---\n",
    "    # Stratified split requires at least 2 members per class.\n",
    "    value_counts = y.value_counts()\n",
    "    single_sample_classes = value_counts[value_counts < 2].index\n",
    "\n",
    "    if not single_sample_classes.empty:\n",
    "        print(f\"Warning for target '{target_name}': Removing classes with only 1 sample: {list(single_sample_classes)}\")\n",
    "        original_count = len(y)\n",
    "        mask = ~y.isin(single_sample_classes)\n",
    "        X = X[mask].copy()\n",
    "        y = y[mask].copy()\n",
    "        print(f\"Removed {original_count - len(y)} rows.\")\n",
    "\n",
    "    if y.nunique() < 2:\n",
    "        print(f\"Skipping '{target_name}' because it has fewer than 2 valid classes after cleaning.\\n\")\n",
    "        return\n",
    "\n",
    "    # Identify categorical and numerical features from the provided X\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough' # Keep other columns if any\n",
    "    )\n",
    "\n",
    "    # Convert target variable to categorical codes for the model\n",
    "    y_series = pd.Series(y).astype('category')\n",
    "    y_codes = y_series.cat.codes\n",
    "    class_names = y_series.cat.categories.tolist()\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_codes, test_size=0.2, random_state=42, stratify=y_codes)\n",
    "\n",
    "    # Apply preprocessing\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Handle class imbalance using SMOTE on the training data only\n",
    "    min_class_samples = pd.Series(y_train).value_counts().min()\n",
    "    if y_series.nunique() > 1 and min_class_samples > 1:\n",
    "        k_neighbors = min(5, min_class_samples - 1)\n",
    "        print(f\"Applying SMOTE... Smallest class in training set has {min_class_samples} samples. Using k_neighbors={k_neighbors}.\")\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "    else:\n",
    "        print(f\"Skipping SMOTE for '{target_name}': smallest class in training set has {min_class_samples} sample(s).\")\n",
    "        X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "\n",
    "    # --- Build the Neural Network Model ---\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train_resampled.shape[1],)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # --- Train the Model ---\n",
    "    print(\"Training the model...\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model.fit(X_train_resampled, y_train_resampled,\n",
    "              epochs=100,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[early_stopping],\n",
    "              verbose=0) # Set to 1 to see training progress\n",
    "\n",
    "    # --- Save the Trained Model ---\n",
    "    model_filename = f'model_{target_name}.h5'\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    # --- Evaluate the Model ---\n",
    "    y_pred_proba = model.predict(X_test_processed)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "    # Define the full range of possible class labels\n",
    "    all_class_labels = range(len(class_names))\n",
    "    # Create the confusion matrix using all possible labels to ensure correct shape\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=all_class_labels)\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # --- Save Confusion Matrix as SVG ---\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {target_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    svg_filename = f'confusion_matrix_{target_name}.svg'\n",
    "    plt.savefig(svg_filename, format='svg')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix plot saved as {svg_filename}\")\n",
    "\n",
    "    # --- Write Confusion Matrix to the shared Excel file ---\n",
    "    # The sheet name is sanitized to be valid\n",
    "    safe_sheet_name = f'CM_{target_name[:25]}'\n",
    "    cm_df.to_excel(excel_writer, sheet_name=safe_sheet_name)\n",
    "    print(f\"Confusion matrix data added to Excel sheet: '{safe_sheet_name}'\")\n",
    "\n",
    "    # --- Save individual detailed report to a separate Excel file ---\n",
    "    report_filename = f'report_{target_name}.xlsx'\n",
    "    results_df = X_test.copy()\n",
    "    results_df['actual_outcome'] = y.loc[X_test.index]\n",
    "    results_df['predicted_outcome'] = [class_names[i] for i in y_pred]\n",
    "    results_df.to_excel(report_filename, sheet_name='Test_Inputs_and_Predictions')\n",
    "    print(f\"Detailed report saved as {report_filename}\")\n",
    "    print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The data file '{DATA_FILE}' was not found.\")\n",
    "        print(\"Please ensure the file path is correct.\")\n",
    "        exit()\n",
    "\n",
    "    # Drop rows where any of the target columns are missing\n",
    "    df.dropna(subset=OUTPUT_COLUMNS, inplace=True)\n",
    "\n",
    "    # Create a copy of input columns to modify\n",
    "    processed_input_cols = INPUT_COLUMNS.copy()\n",
    "\n",
    "    # Convert date/time columns to numerical features\n",
    "    for col in ['incident_date', 'incident_time']:\n",
    "        if col in df.columns and col in processed_input_cols:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            df[f'{col}_year'] = df[col].dt.year\n",
    "            df[f'{col}_month'] = df[col].dt.month\n",
    "            df[f'{col}_day'] = df[col].dt.day\n",
    "            new_cols = [f'{col}_year', f'{col}_month', f'{col}_day']\n",
    "            if col == 'incident_time':\n",
    "                df[f'{col}_hour'] = df[col].dt.hour\n",
    "                new_cols.append(f'{col}_hour')\n",
    "            \n",
    "            # Update the list of columns to be used as inputs\n",
    "            processed_input_cols.remove(col)\n",
    "            processed_input_cols.extend(new_cols)\n",
    "    \n",
    "    # Drop rows with NaT in date columns after coercion\n",
    "    df.dropna(subset=processed_input_cols, inplace=True)\n",
    "    \n",
    "    # Define X *after* processing columns\n",
    "    # Ensure all columns exist in the dataframe before selection\n",
    "    final_input_cols = [col for col in processed_input_cols if col in df.columns]\n",
    "    X = df[final_input_cols]\n",
    "\n",
    "    # Create a single Excel writer for all confusion matrices\n",
    "    excel_cm_filename = 'all_confusion_matrices.xlsx'\n",
    "    with pd.ExcelWriter(excel_cm_filename, engine='openpyxl') as writer:\n",
    "        print(f\"Starting model training loop. All confusion matrices will be saved in '{excel_cm_filename}'.\\n\")\n",
    "        # Loop through each target variable and train a model\n",
    "        for target in OUTPUT_COLUMNS:\n",
    "            if df[target].nunique() < 2:\n",
    "                print(f\"Skipping '{target}' because it has less than 2 unique values.\")\n",
    "                continue\n",
    "            y = df[target]\n",
    "            # Pass the writer object to the function\n",
    "            train_and_evaluate_model(X, y, target, writer)\n",
    "\n",
    "    print(\"✅ All models have been trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33161f5c-258c-4a4b-a6f0-6a2d270e54ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing target: incident_type ---\n",
      "Warning for target 'incident_type': Removing classes with only 1 sample: [2]\n",
      "Removed 1 rows.\n",
      "Applying SMOTE... Using k_neighbors=5.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as incident_type.h5\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "Confusion matrix plot saved as confusion_matrix_incident_type.svg\n",
      "✅ Added binary classification results for 'incident_type' to summary.\n",
      "Detailed report saved as report_incident_type.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_1 ---\n",
      "Warning for target 'incident_mechanism_1': Removing classes with only 1 sample: [27, 28, 29]\n",
      "Removed 3 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as incident_mechanism_1.h5\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_1.svg\n",
      "ℹ️ Skipping summary for 'incident_mechanism_1' (not a binary classification).\n",
      "Detailed report saved as report_incident_mechanism_1.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_2 ---\n",
      "Warning for target 'incident_mechanism_2': Removing classes with only 1 sample: [13, 22, 23, 26, 27]\n",
      "Removed 5 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as incident_mechanism_2.h5\n",
      "9/9 [==============================] - 0s 997us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_2.svg\n",
      "ℹ️ Skipping summary for 'incident_mechanism_2' (not a binary classification).\n",
      "Detailed report saved as report_incident_mechanism_2.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_mechanism_3 ---\n",
      "Warning for target 'incident_mechanism_3': Removing classes with only 1 sample: [8, 10, 13, 15, 16, 17, 18]\n",
      "Removed 7 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as incident_mechanism_3.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_3.svg\n",
      "ℹ️ Skipping summary for 'incident_mechanism_3' (not a binary classification).\n",
      "Detailed report saved as report_incident_mechanism_3.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: eap_enacted_y_n_due_to_incident ---\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as eap_enacted_y_n_due_to_incident.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_eap_enacted_y_n_due_to_incident.svg\n",
      "ℹ️ Skipping summary for 'eap_enacted_y_n_due_to_incident' (not a binary classification).\n",
      "Detailed report saved as report_eap_enacted_y_n_due_to_incident.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: fatalities_number ---\n",
      "Warning for target 'fatalities_number': Removing classes with only 1 sample: [16, 22, 21, 20, 18, 12, 13, 11, 10, 9, 6, 5, 4, 24]\n",
      "Removed 14 rows.\n",
      "Skipping SMOTE for 'fatalities_number'.\n",
      "Training the model...\n",
      "Model saved as fatalities_number.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_fatalities_number.svg\n",
      "ℹ️ Skipping summary for 'fatalities_number' (not a binary classification).\n",
      "Detailed report saved as report_fatalities_number.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: number_of_people_evacuated ---\n",
      "Warning for target 'number_of_people_evacuated': Removing classes with only 1 sample: [22, 23, 26, 24, 25, 20, 27, 28, 29, 21, 15, 19, 18, 16, 12, 11, 10, 8, 7, 4, 30]\n",
      "Removed 21 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as number_of_people_evacuated.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_number_of_people_evacuated.svg\n",
      "ℹ️ Skipping summary for 'number_of_people_evacuated' (not a binary classification).\n",
      "Detailed report saved as report_number_of_people_evacuated.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: number_of_habitable_structures_evacuated ---\n",
      "Warning for target 'number_of_habitable_structures_evacuated': Removing classes with only 1 sample: [3, 5, 10, 12, 13]\n",
      "Removed 5 rows.\n",
      "Skipping SMOTE for 'number_of_habitable_structures_evacuated'.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "import os\n",
    "\n",
    "# Try to import imblearn, provide install instructions if it fails\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except ImportError:\n",
    "    print(\"Error: The 'imbalanced-learn' library is required but not installed.\")\n",
    "    print(\"Please install it by running the following command in your terminal:\")\n",
    "    print(\"pip install imbalanced-learn\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = '../encoded_dam_data.csv' # Make sure this path is correct\n",
    "INPUT_COLUMNS = [\n",
    "    'state', 'downstream_hazard_potential', 'incident_date', 'incident_time',\n",
    "    'incident_driver', 'owner_type', 'dam_type', 'primary_purpose_s', 'eap',\n",
    "    'dam_height', 'max_storage_ac_ft', 'surface_area_acres', 'year_completed',\n",
    "    'latitude', 'longitude', 'year_modified'\n",
    "]\n",
    "OUTPUT_COLUMNS = [\n",
    "    'incident_type', 'incident_mechanism_1', 'incident_mechanism_2',\n",
    "    'incident_mechanism_3', 'eap_enacted_y_n_due_to_incident',\n",
    "    'fatalities_number', 'number_of_people_evacuated',\n",
    "    'number_of_habitable_structures_evacuated',\n",
    "    'number_of_habitable_structures_flooded', 'other_infrastructure_impacts',\n",
    "    'response', 'volume_released_at_failure_ac_ft', 'incident_duration',\n",
    "    'incident_report_produced'\n",
    "]\n",
    "\n",
    "# --- Main Processing Function ---\n",
    "def train_and_evaluate_model(X, y, target_name, summary_list):\n",
    "    \"\"\"\n",
    "    Trains a neural network and generates evaluation files.\n",
    "    If the target is binary, it appends a summary to the summary_list.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing target: {target_name} ---\")\n",
    "\n",
    "    # --- Pre-split Data Cleaning for Stratification ---\n",
    "    value_counts = y.value_counts()\n",
    "    single_sample_classes = value_counts[value_counts < 2].index\n",
    "\n",
    "    if not single_sample_classes.empty:\n",
    "        print(f\"Warning for target '{target_name}': Removing classes with only 1 sample: {list(single_sample_classes)}\")\n",
    "        original_count = len(y)\n",
    "        mask = ~y.isin(single_sample_classes)\n",
    "        X = X[mask].copy()\n",
    "        y = y[mask].copy()\n",
    "        print(f\"Removed {original_count - len(y)} rows.\")\n",
    "\n",
    "    if y.nunique() < 2:\n",
    "        print(f\"Skipping '{target_name}' because it has fewer than 2 valid classes after cleaning.\\n\")\n",
    "        return\n",
    "\n",
    "    # Identify features\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Convert target to codes\n",
    "    y_series = pd.Series(y).astype('category')\n",
    "    y_codes = y_series.cat.codes\n",
    "    class_names = y_series.cat.categories.tolist()\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_codes, test_size=0.2, random_state=42, stratify=y_codes)\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Handle class imbalance using SMOTE\n",
    "    min_class_samples = pd.Series(y_train).value_counts().min()\n",
    "    if y_series.nunique() > 1 and min_class_samples > 1:\n",
    "        k_neighbors = min(5, min_class_samples - 1)\n",
    "        print(f\"Applying SMOTE... Using k_neighbors={k_neighbors}.\")\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "    else:\n",
    "        print(f\"Skipping SMOTE for '{target_name}'.\")\n",
    "        X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "\n",
    "    # --- Build and Train Model ---\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train_resampled.shape[1],)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Training the model...\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model.fit(X_train_resampled, y_train_resampled, epochs=100, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # --- Save Model (Named by output) ---\n",
    "    model_filename = f'{target_name}.h5'\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    # --- Evaluate Model and Create Confusion Matrix ---\n",
    "    y_pred = np.argmax(model.predict(X_test_processed), axis=1)\n",
    "    all_class_labels = range(len(class_names))\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=all_class_labels)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # --- Save Confusion Matrix Plot ---\n",
    "    svg_filename = f'confusion_matrix_{target_name}.svg'\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {target_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(svg_filename, format='svg')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix plot saved as {svg_filename}\")\n",
    "\n",
    "    # --- Add results to the summary report if classification is binary ---\n",
    "    if len(class_names) == 2:\n",
    "        # For a 2x2 matrix, ravel() provides [TN, FP, FN, TP]\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        summary_result = {\n",
    "            'Output Name': target_name,\n",
    "            'Actual 1, Predicted 1 (TP)': tp,\n",
    "            'Actual 0, Predicted 0 (TN)': tn,\n",
    "            'Actual 0, Predicted 1 (FP)': fp,\n",
    "            'Actual 1, Predicted 0 (FN)': fn\n",
    "        }\n",
    "        summary_list.append(summary_result)\n",
    "        print(f\"✅ Added binary classification results for '{target_name}' to summary.\")\n",
    "    else:\n",
    "        print(f\"ℹ️ Skipping summary for '{target_name}' (not a binary classification).\")\n",
    "\n",
    "    # --- Save detailed individual report ---\n",
    "    report_filename = f'report_{target_name}.xlsx'\n",
    "    results_df = X_test.copy()\n",
    "    results_df['actual_outcome'] = y.loc[X_test.index]\n",
    "    results_df['predicted_outcome'] = [class_names[i] for i in y_pred]\n",
    "    results_df.to_excel(report_filename, sheet_name='Test_Inputs_and_Predictions')\n",
    "    print(f\"Detailed report saved as {report_filename}\")\n",
    "    print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The data file '{DATA_FILE}' was not found.\")\n",
    "        exit()\n",
    "\n",
    "    df.dropna(subset=OUTPUT_COLUMNS, inplace=True)\n",
    "    \n",
    "    # Process date/time columns\n",
    "    processed_input_cols = INPUT_COLUMNS.copy()\n",
    "    for col in ['incident_date', 'incident_time']:\n",
    "        if col in df.columns and col in processed_input_cols:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            df[f'{col}_year'] = df[col].dt.year\n",
    "            df[f'{col}_month'] = df[col].dt.month\n",
    "            df[f'{col}_day'] = df[col].dt.day\n",
    "            new_cols = [f'{col}_year', f'{col}_month', f'{col}_day']\n",
    "            if col == 'incident_time':\n",
    "                df[f'{col}_hour'] = df[col].dt.hour\n",
    "                new_cols.append(f'{col}_hour')\n",
    "            processed_input_cols.remove(col)\n",
    "            processed_input_cols.extend(new_cols)\n",
    "    \n",
    "    df.dropna(subset=processed_input_cols, inplace=True)\n",
    "    \n",
    "    final_input_cols = [col for col in processed_input_cols if col in df.columns]\n",
    "    X = df[final_input_cols]\n",
    "\n",
    "    # --- Initialize a list to hold summary results for binary models ---\n",
    "    classification_summary_data = []\n",
    "\n",
    "    # Loop through each target variable and train a model\n",
    "    for target in OUTPUT_COLUMNS:\n",
    "        if df[target].nunique() < 2:\n",
    "            print(f\"Skipping '{target}' because it has less than 2 unique values.\")\n",
    "            continue\n",
    "        y = df[target]\n",
    "        train_and_evaluate_model(X, y, target, classification_summary_data)\n",
    "\n",
    "    # --- Save the consolidated binary classification summary to one Excel file ---\n",
    "    if classification_summary_data:\n",
    "        summary_df = pd.DataFrame(classification_summary_data)\n",
    "        summary_filename = 'binary_classification_summary.xlsx'\n",
    "        summary_df.to_excel(summary_filename, index=False)\n",
    "        print(f\"✅ All models trained. Binary summary saved to '{summary_filename}'.\")\n",
    "    else:\n",
    "        print(\"✅ All models trained. No binary classification tasks were run, so no summary file was created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e95e4dab-2d6b-4ecc-8912-02291cb76b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing target: incident_type ---\n",
      "Warning for target 'incident_type': Removing classes with only 1 sample: [2]\n",
      "Removed 1 rows.\n",
      "Applying SMOTE... Using k_neighbors=5.\n",
      "Training the model...\n",
      "Model saved as incident_type.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_type.svg\n",
      "✅ Binary results for 'incident_type' added.\n",
      "--- Processing target: incident_mechanism_1 ---\n",
      "Warning for target 'incident_mechanism_1': Removing classes with only 1 sample: [27, 28, 29]\n",
      "Removed 3 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as incident_mechanism_1.h5\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_1.svg\n",
      "ℹ️ Skipping summary for 'incident_mechanism_1' (not binary).\n",
      "--- Processing target: incident_mechanism_2 ---\n",
      "Warning for target 'incident_mechanism_2': Removing classes with only 1 sample: [13, 22, 23, 26, 27]\n",
      "Removed 5 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as incident_mechanism_2.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_2.svg\n",
      "ℹ️ Skipping summary for 'incident_mechanism_2' (not binary).\n",
      "--- Processing target: incident_mechanism_3 ---\n",
      "Warning for target 'incident_mechanism_3': Removing classes with only 1 sample: [8, 10, 13, 15, 16, 17, 18]\n",
      "Removed 7 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as incident_mechanism_3.h5\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_incident_mechanism_3.svg\n",
      "ℹ️ Skipping summary for 'incident_mechanism_3' (not binary).\n",
      "--- Processing target: eap_enacted_y_n_due_to_incident ---\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as eap_enacted_y_n_due_to_incident.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_eap_enacted_y_n_due_to_incident.svg\n",
      "ℹ️ Skipping summary for 'eap_enacted_y_n_due_to_incident' (not binary).\n",
      "--- Processing target: fatalities_number ---\n",
      "Warning for target 'fatalities_number': Removing classes with only 1 sample: [16, 22, 21, 20, 18, 12, 13, 11, 10, 9, 6, 5, 4, 24]\n",
      "Removed 14 rows.\n",
      "Skipping SMOTE for 'fatalities_number'.\n",
      "Training the model...\n",
      "Model saved as fatalities_number.h5\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as confusion_matrix_fatalities_number.svg\n",
      "ℹ️ Skipping summary for 'fatalities_number' (not binary).\n",
      "--- Processing target: number_of_people_evacuated ---\n",
      "Warning for target 'number_of_people_evacuated': Removing classes with only 1 sample: [22, 23, 26, 24, 25, 20, 27, 28, 29, 21, 15, 19, 18, 16, 12, 11, 10, 8, 7, 4, 30]\n",
      "Removed 21 rows.\n",
      "Applying SMOTE... Using k_neighbors=1.\n",
      "Training the model...\n",
      "Model saved as number_of_people_evacuated.h5\n",
      "1/9 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step\n",
      "Confusion matrix plot saved as confusion_matrix_number_of_people_evacuated.svg\n",
      "ℹ️ Skipping summary for 'number_of_people_evacuated' (not binary).\n",
      "--- Processing target: number_of_habitable_structures_evacuated ---\n",
      "Warning for target 'number_of_habitable_structures_evacuated': Removing classes with only 1 sample: [3, 5, 10, 12, 13]\n",
      "Removed 5 rows.\n",
      "Skipping SMOTE for 'number_of_habitable_structures_evacuated'.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "import os\n",
    "\n",
    "# Try to import imblearn\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except ImportError:\n",
    "    print(\"Error: The 'imbalanced-learn' library is required but not installed.\")\n",
    "    print(\"Please install it by running the following command in your terminal:\")\n",
    "    print(\"pip install imbalanced-learn\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = '../encoded_dam_data.csv'  # Make sure this path is correct\n",
    "INPUT_COLUMNS = [\n",
    "    'state', 'downstream_hazard_potential', 'incident_date', 'incident_time',\n",
    "    'incident_driver', 'owner_type', 'dam_type', 'primary_purpose_s', 'eap',\n",
    "    'dam_height', 'max_storage_ac_ft', 'surface_area_acres', 'year_completed',\n",
    "    'latitude', 'longitude', 'year_modified'\n",
    "]\n",
    "OUTPUT_COLUMNS = [\n",
    "    'incident_type', 'incident_mechanism_1', 'incident_mechanism_2',\n",
    "    'incident_mechanism_3', 'eap_enacted_y_n_due_to_incident',\n",
    "    'fatalities_number', 'number_of_people_evacuated',\n",
    "    'number_of_habitable_structures_evacuated',\n",
    "    'number_of_habitable_structures_flooded', 'other_infrastructure_impacts',\n",
    "    'response', 'volume_released_at_failure_ac_ft', 'incident_duration',\n",
    "    'incident_report_produced'\n",
    "]\n",
    "\n",
    "# --- Main Processing Function ---\n",
    "def train_and_evaluate_model(X, y, target_name, summary_list):\n",
    "    \"\"\"\n",
    "    Trains a neural network and generates evaluation files.\n",
    "    If the target is binary, it appends a summary to the summary_list.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing target: {target_name} ---\")\n",
    "\n",
    "    # --- Pre-split Data Cleaning ---\n",
    "    value_counts = y.value_counts()\n",
    "    single_sample_classes = value_counts[value_counts < 2].index\n",
    "\n",
    "    if not single_sample_classes.empty:\n",
    "        print(f\"Warning for target '{target_name}': Removing classes with only 1 sample: {list(single_sample_classes)}\")\n",
    "        original_count = len(y)\n",
    "        mask = ~y.isin(single_sample_classes)\n",
    "        X = X[mask].copy()\n",
    "        y = y[mask].copy()\n",
    "        print(f\"Removed {original_count - len(y)} rows.\")\n",
    "\n",
    "    if y.nunique() < 2:\n",
    "        print(f\"Skipping '{target_name}' because it has fewer than 2 valid classes after cleaning.\\n\")\n",
    "        return\n",
    "\n",
    "    # Identify features\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "    # Preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Target encoding\n",
    "    y_series = pd.Series(y).astype('category')\n",
    "    y_codes = y_series.cat.codes\n",
    "    class_names = y_series.cat.categories.tolist()\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_codes, test_size=0.2, random_state=42, stratify=y_codes\n",
    "    )\n",
    "\n",
    "    # Preprocess\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Handle imbalance\n",
    "    min_class_samples = pd.Series(y_train).value_counts().min()\n",
    "    if y_series.nunique() > 1 and min_class_samples > 1:\n",
    "        k_neighbors = min(5, min_class_samples - 1)\n",
    "        print(f\"Applying SMOTE... Using k_neighbors={k_neighbors}.\")\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "    else:\n",
    "        print(f\"Skipping SMOTE for '{target_name}'.\")\n",
    "        X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "\n",
    "    # --- Build Model ---\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train_resampled.shape[1],)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model.fit(X_train_resampled, y_train_resampled, epochs=100,\n",
    "              validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # --- Save Model by Output Name ---\n",
    "    model_filename = f'{target_name}.h5'\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    y_pred = np.argmax(model.predict(X_test_processed), axis=1)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=range(len(class_names)))\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # Save confusion matrix plot\n",
    "    svg_filename = f'confusion_matrix_{target_name}.svg'\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {target_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(svg_filename, format='svg')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix plot saved as {svg_filename}\")\n",
    "\n",
    "    # --- Save Binary Summary ---\n",
    "    if len(class_names) == 2:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        summary_list.append({\n",
    "            'Output Name': target_name,\n",
    "            'Actual 1, Predicted 1 (TP)': tp,\n",
    "            'Actual 0, Predicted 0 (TN)': tn,\n",
    "            'Actual 0, Predicted 1 (FP)': fp,\n",
    "            'Actual 1, Predicted 0 (FN)': fn\n",
    "        })\n",
    "        print(f\"✅ Binary results for '{target_name}' added.\")\n",
    "    else:\n",
    "        print(f\"ℹ️ Skipping summary for '{target_name}' (not binary).\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The data file '{DATA_FILE}' was not found.\")\n",
    "        exit()\n",
    "\n",
    "    df.dropna(subset=OUTPUT_COLUMNS, inplace=True)\n",
    "\n",
    "    # Process date/time\n",
    "    processed_input_cols = INPUT_COLUMNS.copy()\n",
    "    for col in ['incident_date', 'incident_time']:\n",
    "        if col in df.columns and col in processed_input_cols:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            df[f'{col}_year'] = df[col].dt.year\n",
    "            df[f'{col}_month'] = df[col].dt.month\n",
    "            df[f'{col}_day'] = df[col].dt.day\n",
    "            new_cols = [f'{col}_year', f'{col}_month', f'{col}_day']\n",
    "            if col == 'incident_time':\n",
    "                df[f'{col}_hour'] = df[col].dt.hour\n",
    "                new_cols.append(f'{col}_hour')\n",
    "            processed_input_cols.remove(col)\n",
    "            processed_input_cols.extend(new_cols)\n",
    "\n",
    "    df.dropna(subset=processed_input_cols, inplace=True)\n",
    "\n",
    "    final_input_cols = [col for col in processed_input_cols if col in df.columns]\n",
    "    X = df[final_input_cols]\n",
    "\n",
    "    # Summary results\n",
    "    classification_summary_data = []\n",
    "\n",
    "    # Train/evaluate for each output\n",
    "    for target in OUTPUT_COLUMNS:\n",
    "        if df[target].nunique() < 2:\n",
    "            print(f\"Skipping '{target}' (less than 2 unique values).\")\n",
    "            continue\n",
    "        y = df[target]\n",
    "        train_and_evaluate_model(X, y, target, classification_summary_data)\n",
    "\n",
    "    # Save consolidated summary\n",
    "    if classification_summary_data:\n",
    "        summary_df = pd.DataFrame(classification_summary_data)\n",
    "        summary_filename = 'binary_classification_summary.xlsx'\n",
    "        summary_df.to_excel(summary_filename, index=False)\n",
    "        print(f\"✅ All binary summaries saved to '{summary_filename}'.\")\n",
    "    else:\n",
    "        print(\"✅ No binary outputs, no summary file created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fd516-e5b8-464e-9cf7-120d6351fbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
