{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3926c-f132-4fd9-888c-7d4ffb8dc9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrading the model3 by adding three more blocks to our nerual network structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c2fc40-dd6f-4010-ae51-caf687b985cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "--- Processing target: dam_height | Task Type: REGRESSION ---\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Training the model...\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Training complete.\n",
      "9/9 [==============================] - 2s 13ms/step\n",
      "✅ Performance metrics for 'dam_height' collected.\n",
      "Plot saved as plot_dam_height.svg\n",
      "Detailed report saved as report_dam_height.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: max_storage_ac_ft | Task Type: REGRESSION ---\n",
      "Training the model...\n",
      "Training complete.\n",
      "9/9 [==============================] - 2s 8ms/step\n",
      "✅ Performance metrics for 'max_storage_ac_ft' collected.\n",
      "Plot saved as plot_max_storage_ac_ft.svg\n",
      "Detailed report saved as report_max_storage_ac_ft.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: surface_area_acres | Task Type: REGRESSION ---\n",
      "Training the model...\n",
      "Training complete.\n",
      "9/9 [==============================] - 1s 4ms/step\n",
      "✅ Performance metrics for 'surface_area_acres' collected.\n",
      "Plot saved as plot_surface_area_acres.svg\n",
      "Detailed report saved as report_surface_area_acres.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_date_year | Task Type: REGRESSION ---\n",
      "Training the model...\n",
      "Training complete.\n",
      "9/9 [==============================] - 1s 5ms/step\n",
      "✅ Performance metrics for 'incident_date_year' collected.\n",
      "Plot saved as plot_incident_date_year.svg\n",
      "Detailed report saved as report_incident_date_year.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_date_month | Task Type: REGRESSION ---\n",
      "Training the model...\n",
      "Training complete.\n",
      "9/9 [==============================] - 1s 3ms/step\n",
      "✅ Performance metrics for 'incident_date_month' collected.\n",
      "Plot saved as plot_incident_date_month.svg\n",
      "Detailed report saved as report_incident_date_month.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_date_day | Task Type: REGRESSION ---\n",
      "Training the model...\n",
      "Training complete.\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "✅ Performance metrics for 'incident_date_day' collected.\n",
      "Plot saved as plot_incident_date_day.svg\n",
      "Detailed report saved as report_incident_date_day.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_time_hour | Task Type: REGRESSION ---\n",
      "Training the model...\n",
      "Training complete.\n",
      "9/9 [==============================] - 1s 4ms/step\n",
      "✅ Performance metrics for 'incident_time_hour' collected.\n",
      "Plot saved as plot_incident_time_hour.svg\n",
      "Detailed report saved as report_incident_time_hour.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: number_of_people_evacuated | Task Type: REGRESSION ---\n",
      "Training the model...\n",
      "Training complete.\n",
      "9/9 [==============================] - 1s 3ms/step\n",
      "✅ Performance metrics for 'number_of_people_evacuated' collected.\n",
      "Plot saved as plot_number_of_people_evacuated.svg\n",
      "Detailed report saved as report_number_of_people_evacuated.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: number_of_habitable_structures_evacuated | Task Type: REGRESSION ---\n",
      "Training the model...\n",
      "Training complete.\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "✅ Performance metrics for 'number_of_habitable_structures_evacuated' collected.\n",
      "Plot saved as plot_number_of_habitable_structures_evacuated.svg\n",
      "Detailed report saved as report_number_of_habitable_structures_evacuated.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: number_of_habitable_structures_flooded | Task Type: REGRESSION ---\n",
      "Training the model...\n",
      "Training complete.\n",
      "9/9 [==============================] - 1s 3ms/step\n",
      "✅ Performance metrics for 'number_of_habitable_structures_flooded' collected.\n",
      "Plot saved as plot_number_of_habitable_structures_flooded.svg\n",
      "Detailed report saved as report_number_of_habitable_structures_flooded.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: volume_released_at_failure_ac_ft | Task Type: REGRESSION ---\n",
      "Training the model...\n",
      "Training complete.\n",
      "9/9 [==============================] - 1s 3ms/step\n",
      "✅ Performance metrics for 'volume_released_at_failure_ac_ft' collected.\n",
      "Plot saved as plot_volume_released_at_failure_ac_ft.svg\n",
      "Detailed report saved as report_volume_released_at_failure_ac_ft.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing target: incident_duration | Task Type: REGRESSION ---\n",
      "Training the model...\n",
      "Training complete.\n",
      "9/9 [==============================] - 1s 3ms/step\n",
      "✅ Performance metrics for 'incident_duration' collected.\n",
      "Plot saved as plot_incident_duration.svg\n",
      "Detailed report saved as report_incident_duration.xlsx\n",
      "----------------------------------------\n",
      "\n",
      "✅ All performance metrics saved to 'model_performance_metrics.xlsx'.\n",
      "\n",
      "All tasks complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = '../regression_data.csv' # Make sure this path is correct\n",
    "\n",
    "INPUT_COLUMNS = [\n",
    "    'state', 'downstream_hazard_potential', 'owner_type', 'dam_type',\n",
    "    'primary_purpose_s', 'eap', 'year_completed', 'latitude', 'longitude', 'year_modified'\n",
    "]\n",
    "\n",
    "NEW_TARGET_COLUMNS = [\n",
    "    'dam_height', 'max_storage_ac_ft', 'surface_area_acres',\n",
    "    'incident_date_year', 'incident_date_month', 'incident_date_day',\n",
    "    'incident_time_hour', 'number_of_people_evacuated',\n",
    "    'number_of_habitable_structures_evacuated',\n",
    "    'number_of_habitable_structures_flooded',\n",
    "    'volume_released_at_failure_ac_ft', 'incident_duration'\n",
    "]\n",
    "\n",
    "# --- Training Configuration ---\n",
    "EPOCHS = 150\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "# --- UPGRADED: Added patience for learning rate reduction ---\n",
    "REDUCE_LR_PATIENCE = 5 \n",
    "\n",
    "# --- UPGRADED: Model Building Function (More Powerful Architecture) ---\n",
    "def build_model(input_shape):\n",
    "    \"\"\"Builds an upgraded, more powerful regression model.\"\"\"\n",
    "    # --- Define Hyperparameters ---\n",
    "    units_1 = 512\n",
    "    units_2 = 256\n",
    "    units_3 = 128\n",
    "    l2_reg = 0.001\n",
    "    dropout_1 = 0.4 # Higher dropout for larger layer\n",
    "    dropout_2 = 0.3\n",
    "    dropout_3 = 0.2 # Lower dropout for smaller layer\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
    "\n",
    "    # Block 1\n",
    "    model.add(tf.keras.layers.Dense(units=units_1, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n",
    "    # --- UPGRADED: Using LeakyReLU activation ---\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.1)) \n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_1))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(tf.keras.layers.Dense(units=units_2, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_2))\n",
    "    \n",
    "    # --- UPGRADED: Added a third dense block ---\n",
    "    # Block 3\n",
    "    model.add(tf.keras.layers.Dense(units=units_3, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_3))\n",
    "\n",
    "        # Block 4\n",
    "    model.add(tf.keras.layers.Dense(units=units_3, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_3))\n",
    "\n",
    "\n",
    "\n",
    "        # Block 5\n",
    "    model.add(tf.keras.layers.Dense(units=units_3, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_3))\n",
    "\n",
    "\n",
    "\n",
    "        # Block 6\n",
    "    model.add(tf.keras.layers.Dense(units=units_3, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_3))\n",
    "\n",
    "    # Output Layer for Regression\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- Main Processing Function ---\n",
    "def train_and_evaluate_model(X, y, target_name, metrics_list):\n",
    "    \"\"\"Trains a regression neural network and collects performance metrics.\"\"\"\n",
    "    print(f\"--- Processing target: {target_name} | Task Type: REGRESSION ---\")\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # --- Train-Test Split ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # --- Standard Model Training ---\n",
    "    input_shape = (X_train_processed.shape[1],)\n",
    "    model = build_model(input_shape)\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # --- UPGRADED: Added ReduceLROnPlateau callback ---\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2, # Reduce LR by a factor of 5 (1/5 = 0.2)\n",
    "        patience=REDUCE_LR_PATIENCE,\n",
    "        min_lr=1e-6, # Don't let the learning rate get too small\n",
    "        verbose=0 # Set to 1 to see messages when LR is reduced\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_processed,\n",
    "        y_train,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=0.2,\n",
    "        # --- UPGRADED: Pass both callbacks to the model ---\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0  # Set to 1 if you want to see training progress\n",
    "    )\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # --- Evaluation & Reporting ---\n",
    "    y_pred = model.predict(X_test_processed).flatten()\n",
    "\n",
    "    # Calculate standard dimensional metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Calculate standard non-dimensional metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    evs = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate Adjusted R-squared\n",
    "    n = X_test_processed.shape[0]\n",
    "    p = X_test_processed.shape[1]\n",
    "    if n - p - 1 > 0:\n",
    "        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    else:\n",
    "        adj_r2 = np.nan\n",
    "\n",
    "    # Calculate Mean Absolute Percentage Error (MAPE)\n",
    "    y_test_non_zero = y_test[y_test != 0]\n",
    "    y_pred_non_zero = y_pred[y_test != 0]\n",
    "    if len(y_test_non_zero) > 0:\n",
    "        mape = np.mean(np.abs((y_test_non_zero - y_pred_non_zero) / y_test_non_zero)) * 100\n",
    "    else:\n",
    "        mape = np.nan\n",
    "    \n",
    "    metrics_result = {\n",
    "        'Model Output': target_name,\n",
    "        'Task Type': 'Regression',\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'R2_Score': r2,\n",
    "        'Adjusted_R2_Score': adj_r2,\n",
    "        'Explained_Variance_Score': evs,\n",
    "        'MAPE (%)': mape\n",
    "    }\n",
    "    \n",
    "    metrics_list.append(metrics_result)\n",
    "    print(f\"✅ Performance metrics for '{target_name}' collected.\")\n",
    "\n",
    "    # --- Generate Actual vs. Predicted Plot ---\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2)\n",
    "    plt.title(f'Actual vs. Predicted for {target_name}')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    \n",
    "    plot_filename = f'plot_{target_name}.svg'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_filename, format='svg')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved as {plot_filename}\")\n",
    "\n",
    "    # --- Save Detailed Report ---\n",
    "    report_filename = f'report_{target_name}.xlsx'\n",
    "    results_df = X_test.copy()\n",
    "    results_df['actual_outcome'] = y_test\n",
    "    results_df['predicted_outcome'] = y_pred\n",
    "    results_df.to_excel(report_filename, sheet_name='Test_Inputs_and_Predictions', index=False)\n",
    "    print(f\"Detailed report saved as {report_filename}\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The data file '{DATA_FILE}' was not found.\")\n",
    "        exit()\n",
    "\n",
    "    model_metrics_data = []\n",
    "\n",
    "    for target in NEW_TARGET_COLUMNS:\n",
    "        if target not in df.columns:\n",
    "            print(f\"Warning: Target column '{target}' not found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        if not pd.api.types.is_numeric_dtype(df[target]):\n",
    "            print(f\"Warning: Target column '{target}' is not numeric. Skipping regression task.\")\n",
    "            continue\n",
    "\n",
    "        current_inputs = [col for col in INPUT_COLUMNS if col in df.columns]\n",
    "        temp_df = df[current_inputs + [target]].dropna()\n",
    "        \n",
    "        if len(temp_df) < 50:\n",
    "             print(f\"Warning: Too little data for '{target}' after dropping NaNs. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        X_filtered = temp_df[current_inputs]\n",
    "        y = temp_df[target]\n",
    "        \n",
    "        train_and_evaluate_model(X_filtered, y, target, model_metrics_data)\n",
    "\n",
    "    if model_metrics_data:\n",
    "        metrics_df = pd.DataFrame(model_metrics_data)\n",
    "        metrics_filename = 'model_performance_metrics.xlsx'\n",
    "        metrics_df.to_excel(metrics_filename, index=False)\n",
    "        print(f\"✅ All performance metrics saved to '{metrics_filename}'.\")\n",
    "\n",
    "    print(\"\\nAll tasks complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bbca52-3def-486b-bf0a-55813eb10605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
