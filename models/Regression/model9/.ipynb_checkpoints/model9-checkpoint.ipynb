{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e082a5e9-2eb1-4f87-ba12-bb2ddc8bb0eb",
   "metadata": {},
   "source": [
    "# upgrade nerual network from model5 \n",
    "Of course. Your current script is well-structured and uses strong techniques like BatchNormalization and LeakyReLU. To get better results, we can focus on three key areas:\n",
    "\n",
    "Switching to a Multi-Output Model: Instead of training a separate model for each target variable, we can train a single, more efficient model that predicts all of them simultaneously. This allows the model to learn shared patterns in the data that can improve predictions across all outputs.\n",
    "\n",
    "Feature Engineering: We can create more meaningful input features from the existing data. For example, calculating the age of a dam is likely more predictive than just its completion year.\n",
    "\n",
    "Refining the Architecture: Your network is quite deep. We can adjust its shape to a more traditional \"funnel\" structure, which can sometimes improve the flow of information.\n",
    "\n",
    "Here is the upgraded code incorporating these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34773dbd-ee4b-4f69-ae70-43e7a674b071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Engineering new features...\n",
      "âœ… Feature engineering complete.\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               5632      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 512)               2048      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 12)                780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182732 (713.80 KB)\n",
      "Trainable params: 180812 (706.30 KB)\n",
      "Non-trainable params: 1920 (7.50 KB)\n",
      "_________________________________________________________________\n",
      "\n",
      "--- Training the multi-output model... ---\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "27/27 [==============================] - 75s 292ms/step - loss: 1609299584.0000 - mae: 1160.9672 - val_loss: 2307321344.0000 - val_mae: 1358.9725 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 1s 51ms/step - loss: 1609286272.0000 - mae: 1160.8934 - val_loss: 2307316224.0000 - val_mae: 1358.9584 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 1609276928.0000 - mae: 1160.8210 - val_loss: 2307308544.0000 - val_mae: 1358.9421 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 1609265152.0000 - mae: 1160.7358 - val_loss: 2307301376.0000 - val_mae: 1358.9117 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 2s 69ms/step - loss: 1609253760.0000 - mae: 1160.6613 - val_loss: 2307291136.0000 - val_mae: 1359.0574 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 1s 52ms/step - loss: 1609254144.0000 - mae: 1160.5696 - val_loss: 2307288832.0000 - val_mae: 1359.3409 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 1s 56ms/step - loss: 1609241856.0000 - mae: 1160.4703 - val_loss: 2307273728.0000 - val_mae: 1359.5944 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 3s 131ms/step - loss: 1609241344.0000 - mae: 1160.3136 - val_loss: 2307255040.0000 - val_mae: 1359.8010 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 4s 136ms/step - loss: 1609235072.0000 - mae: 1160.1979 - val_loss: 2307273728.0000 - val_mae: 1360.1385 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 3s 131ms/step - loss: 1609209472.0000 - mae: 1160.1283 - val_loss: 2307248896.0000 - val_mae: 1360.5117 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 4s 144ms/step - loss: 1609206912.0000 - mae: 1160.0192 - val_loss: 2307260928.0000 - val_mae: 1361.1821 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 4s 141ms/step - loss: 1609198208.0000 - mae: 1159.8937 - val_loss: 2307217408.0000 - val_mae: 1361.1067 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - 3s 123ms/step - loss: 1609196800.0000 - mae: 1159.6692 - val_loss: 2307168256.0000 - val_mae: 1361.1384 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 4s 141ms/step - loss: 1609182976.0000 - mae: 1159.3677 - val_loss: 2307176192.0000 - val_mae: 1361.1344 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 3s 116ms/step - loss: 1609182208.0000 - mae: 1159.5587 - val_loss: 2307152896.0000 - val_mae: 1361.0494 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 4s 143ms/step - loss: 1609158144.0000 - mae: 1159.2230 - val_loss: 2307119616.0000 - val_mae: 1361.6171 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 3s 106ms/step - loss: 1609146880.0000 - mae: 1158.9464 - val_loss: 2306979328.0000 - val_mae: 1360.5734 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - 3s 108ms/step - loss: 1609137152.0000 - mae: 1158.5739 - val_loss: 2307058944.0000 - val_mae: 1360.6013 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 3s 116ms/step - loss: 1609132544.0000 - mae: 1158.4845 - val_loss: 2306994432.0000 - val_mae: 1360.4305 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "27/27 [==============================] - 3s 116ms/step - loss: 1609122944.0000 - mae: 1157.9323 - val_loss: 2307072256.0000 - val_mae: 1360.0094 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "27/27 [==============================] - 3s 127ms/step - loss: 1609113600.0000 - mae: 1157.7628 - val_loss: 2306979072.0000 - val_mae: 1359.7327 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 1609084160.0000 - mae: 1157.4023 - val_loss: 2306984704.0000 - val_mae: 1358.9873 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - 4s 167ms/step - loss: 1609071104.0000 - mae: 1157.2072 - val_loss: 2306856448.0000 - val_mae: 1358.6934 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 1609084544.0000 - mae: 1156.7235 - val_loss: 2306861056.0000 - val_mae: 1358.0045 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "27/27 [==============================] - 6s 208ms/step - loss: 1609059840.0000 - mae: 1156.8527 - val_loss: 2307009280.0000 - val_mae: 1358.4747 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "27/27 [==============================] - 6s 238ms/step - loss: 1609038592.0000 - mae: 1156.4200 - val_loss: 2307071744.0000 - val_mae: 1357.5477 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "27/27 [==============================] - 5s 193ms/step - loss: 1609025664.0000 - mae: 1155.7529 - val_loss: 2306905088.0000 - val_mae: 1356.6685 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 1609011072.0000 - mae: 1155.7905 - val_loss: 2306746880.0000 - val_mae: 1355.7261 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "27/27 [==============================] - 3s 108ms/step - loss: 1609019136.0000 - mae: 1155.1677 - val_loss: 2306800128.0000 - val_mae: 1355.1664 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "27/27 [==============================] - 4s 146ms/step - loss: 1608968704.0000 - mae: 1154.9514 - val_loss: 2306894848.0000 - val_mae: 1355.2106 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "27/27 [==============================] - 3s 125ms/step - loss: 1608968064.0000 - mae: 1154.2609 - val_loss: 2306700800.0000 - val_mae: 1354.0404 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "27/27 [==============================] - 3s 123ms/step - loss: 1608934656.0000 - mae: 1153.8743 - val_loss: 2306759680.0000 - val_mae: 1354.5593 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 1608943232.0000 - mae: 1153.5817 - val_loss: 2306878976.0000 - val_mae: 1354.4159 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 1608920064.0000 - mae: 1153.4240 - val_loss: 2306456576.0000 - val_mae: 1352.9923 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "27/27 [==============================] - 3s 126ms/step - loss: 1608887040.0000 - mae: 1152.9099 - val_loss: 2306709504.0000 - val_mae: 1352.9160 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "27/27 [==============================] - 3s 125ms/step - loss: 1608857984.0000 - mae: 1152.2676 - val_loss: 2306918656.0000 - val_mae: 1352.6074 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "27/27 [==============================] - 4s 142ms/step - loss: 1608863744.0000 - mae: 1152.2571 - val_loss: 2306712832.0000 - val_mae: 1351.2360 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - 4s 137ms/step - loss: 1608845696.0000 - mae: 1151.6259 - val_loss: 2306611968.0000 - val_mae: 1350.8514 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "27/27 [==============================] - 4s 146ms/step - loss: 1608806400.0000 - mae: 1151.2106 - val_loss: 2306332160.0000 - val_mae: 1349.5513 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - 3s 122ms/step - loss: 1608832128.0000 - mae: 1150.6337 - val_loss: 2306628864.0000 - val_mae: 1349.9086 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "27/27 [==============================] - 3s 108ms/step - loss: 1608803072.0000 - mae: 1150.1394 - val_loss: 2306592000.0000 - val_mae: 1349.1041 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 1608782848.0000 - mae: 1149.8419 - val_loss: 2306496000.0000 - val_mae: 1349.4678 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "27/27 [==============================] - 5s 201ms/step - loss: 1608735360.0000 - mae: 1149.4246 - val_loss: 2305929472.0000 - val_mae: 1346.7174 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "27/27 [==============================] - 5s 191ms/step - loss: 1608768896.0000 - mae: 1148.5305 - val_loss: 2306396416.0000 - val_mae: 1347.2239 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "27/27 [==============================] - 5s 198ms/step - loss: 1608768384.0000 - mae: 1148.2788 - val_loss: 2306185984.0000 - val_mae: 1346.0436 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "27/27 [==============================] - 4s 146ms/step - loss: 1608705792.0000 - mae: 1148.1294 - val_loss: 2306798848.0000 - val_mae: 1346.7754 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "27/27 [==============================] - 4s 136ms/step - loss: 1608657024.0000 - mae: 1147.0529 - val_loss: 2306411776.0000 - val_mae: 1344.2834 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "27/27 [==============================] - 3s 133ms/step - loss: 1608703104.0000 - mae: 1146.4906 - val_loss: 2306489600.0000 - val_mae: 1343.7836 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "27/27 [==============================] - 3s 133ms/step - loss: 1608748800.0000 - mae: 1146.2842 - val_loss: 2306545920.0000 - val_mae: 1343.8416 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 1608704768.0000 - mae: 1145.6814 - val_loss: 2305372160.0000 - val_mae: 1341.2042 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "27/27 [==============================] - 5s 181ms/step - loss: 1608624768.0000 - mae: 1145.2614 - val_loss: 2305581568.0000 - val_mae: 1341.8875 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "27/27 [==============================] - 4s 142ms/step - loss: 1608595584.0000 - mae: 1144.4392 - val_loss: 2305875712.0000 - val_mae: 1341.8575 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "27/27 [==============================] - 4s 135ms/step - loss: 1608574720.0000 - mae: 1144.6707 - val_loss: 2306066688.0000 - val_mae: 1341.7457 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "27/27 [==============================] - 4s 142ms/step - loss: 1608645760.0000 - mae: 1143.6588 - val_loss: 2306328064.0000 - val_mae: 1341.8208 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "27/27 [==============================] - 4s 134ms/step - loss: 1608507392.0000 - mae: 1142.9982 - val_loss: 2306548224.0000 - val_mae: 1342.0183 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "27/27 [==============================] - 3s 125ms/step - loss: 1608507008.0000 - mae: 1142.5188 - val_loss: 2306923776.0000 - val_mae: 1342.9598 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - 3s 129ms/step - loss: 1608508416.0000 - mae: 1142.3538 - val_loss: 2306297600.0000 - val_mae: 1338.9305 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "27/27 [==============================] - 4s 142ms/step - loss: 1608428160.0000 - mae: 1141.1741 - val_loss: 2306395904.0000 - val_mae: 1339.6639 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "27/27 [==============================] - 4s 159ms/step - loss: 1608391040.0000 - mae: 1140.5704 - val_loss: 2306545152.0000 - val_mae: 1340.1013 - lr: 2.0000e-04\n",
      "Epoch 60/200\n",
      "27/27 [==============================] - 4s 145ms/step - loss: 1608469120.0000 - mae: 1140.9965 - val_loss: 2306447360.0000 - val_mae: 1339.8385 - lr: 2.0000e-04\n",
      "Epoch 61/200\n",
      "27/27 [==============================] - 3s 116ms/step - loss: 1608564096.0000 - mae: 1140.7679 - val_loss: 2306309632.0000 - val_mae: 1339.1931 - lr: 2.0000e-04\n",
      "Epoch 62/200\n",
      "27/27 [==============================] - 3s 114ms/step - loss: 1608410624.0000 - mae: 1140.3856 - val_loss: 2306211840.0000 - val_mae: 1339.0981 - lr: 2.0000e-04\n",
      "Epoch 63/200\n",
      "27/27 [==============================] - 3s 115ms/step - loss: 1608424320.0000 - mae: 1140.6614 - val_loss: 2306209280.0000 - val_mae: 1339.0173 - lr: 2.0000e-04\n",
      "Epoch 64/200\n",
      "27/27 [==============================] - 3s 109ms/step - loss: 1608426880.0000 - mae: 1140.3191 - val_loss: 2306447616.0000 - val_mae: 1339.6682 - lr: 2.0000e-04\n",
      "Epoch 65/200\n",
      "27/27 [==============================] - 3s 121ms/step - loss: 1608385152.0000 - mae: 1140.6038 - val_loss: 2306541056.0000 - val_mae: 1339.7662 - lr: 2.0000e-04\n",
      "Epoch 66/200\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 1608365184.0000 - mae: 1140.2018 - val_loss: 2306546944.0000 - val_mae: 1339.2402 - lr: 2.0000e-04\n",
      "Epoch 67/200\n",
      "27/27 [==============================] - 3s 118ms/step - loss: 1608435712.0000 - mae: 1140.4191 - val_loss: 2306557184.0000 - val_mae: 1339.2794 - lr: 4.0000e-05\n",
      "Epoch 68/200\n",
      "27/27 [==============================] - 3s 106ms/step - loss: 1608420352.0000 - mae: 1140.1558 - val_loss: 2306550528.0000 - val_mae: 1339.3591 - lr: 4.0000e-05\n",
      "Epoch 69/200\n",
      "27/27 [==============================] - 2s 88ms/step - loss: 1608398208.0000 - mae: 1139.9318 - val_loss: 2306568448.0000 - val_mae: 1339.3810 - lr: 4.0000e-05\n",
      "Epoch 70/200\n",
      "27/27 [==============================] - 3s 119ms/step - loss: 1608355968.0000 - mae: 1139.6182 - val_loss: 2306577664.0000 - val_mae: 1339.3640 - lr: 4.0000e-05\n",
      "âœ… Training complete.\n",
      "9/9 [==============================] - 4s 16ms/step\n",
      "\n",
      "--- Generating reports and plots for each target ---\n",
      "\n",
      "âœ… All performance metrics saved to 'multi_output_model_performance.xlsx'.\n",
      "âœ… Detailed report saved to 'multi_output_detailed_report.xlsx'.\n",
      "\n",
      "All tasks complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer # Import imputer\n",
    "from sklearn.pipeline import Pipeline # Import pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = '../regression_data.csv' # Make sure this path is correct\n",
    "\n",
    "# Original inputs\n",
    "INPUT_COLUMNS = [\n",
    "    'state', 'downstream_hazard_potential', 'owner_type', 'dam_type',\n",
    "    'primary_purpose_s', 'eap', 'latitude', 'longitude'\n",
    "]\n",
    "\n",
    "# Engineered features will be added to this list\n",
    "FEATURE_ENGINEERED_COLUMNS = [\n",
    "    'dam_age', 'years_since_modified'\n",
    "]\n",
    "\n",
    "# All target variables for the multi-output model\n",
    "TARGET_COLUMNS = [\n",
    "    'dam_height', 'max_storage_ac_ft', 'surface_area_acres',\n",
    "    'incident_date_year', 'incident_date_month', 'incident_date_day',\n",
    "    'incident_time_hour', 'number_of_people_evacuated',\n",
    "    'number_of_habitable_structures_evacuated',\n",
    "    'number_of_habitable_structures_flooded',\n",
    "    'volume_released_at_failure_ac_ft', 'incident_duration'\n",
    "]\n",
    "\n",
    "# --- Training Configuration ---\n",
    "EPOCHS = 200 # Increased epochs for potentially more complex task\n",
    "EARLY_STOPPING_PATIENCE = 20\n",
    "REDUCE_LR_PATIENCE = 8\n",
    "\n",
    "# --- UPGRADED: Multi-Output Model Building Function ---\n",
    "def build_multi_output_model(input_shape, num_outputs):\n",
    "    \"\"\"Builds an upgraded, multi-output regression model with a funnel architecture.\"\"\"\n",
    "    # --- Define Hyperparameters ---\n",
    "    units_1 = 512\n",
    "    units_2 = 256\n",
    "    units_3 = 128\n",
    "    units_4 = 64 # Added a smaller fourth layer\n",
    "    l2_reg = 0.001\n",
    "    dropout_rate = 0.3\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "\n",
    "        # Block 1\n",
    "        tf.keras.layers.Dense(units=units_1, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "\n",
    "        # Block 2\n",
    "        tf.keras.layers.Dense(units=units_2, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        \n",
    "        # Block 3\n",
    "        tf.keras.layers.Dense(units=units_3, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        \n",
    "        # Block 4 (New smaller layer)\n",
    "        tf.keras.layers.Dense(units=units_4, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        # No dropout on the layer before the output\n",
    "\n",
    "        # Output Layer for Multi-Output Regression\n",
    "        tf.keras.layers.Dense(num_outputs, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Use AdamW optimizer which can be more robust with weight decay\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=0.01)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The data file '{DATA_FILE}' was not found.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 1. Feature Engineering ---\n",
    "    print(\"Engineering new features...\")\n",
    "    current_year = datetime.now().year\n",
    "    # Ensure year columns are numeric, coercing errors to NaT (which becomes NaN)\n",
    "    df['year_completed'] = pd.to_numeric(df['year_completed'], errors='coerce')\n",
    "    df['year_modified'] = pd.to_numeric(df['year_modified'], errors='coerce')\n",
    "    \n",
    "    df['dam_age'] = current_year - df['year_completed']\n",
    "    # If year_modified is missing, assume it's the same as completion year\n",
    "    df['years_since_modified'] = current_year - df['year_modified'].fillna(df['year_completed'])\n",
    "    print(\"âœ… Feature engineering complete.\")\n",
    "    \n",
    "    # Define final input features\n",
    "    all_input_features = INPUT_COLUMNS + FEATURE_ENGINEERED_COLUMNS\n",
    "    \n",
    "    # Drop rows where any of the target columns are missing\n",
    "    df_clean = df.dropna(subset=TARGET_COLUMNS)\n",
    "    \n",
    "    if len(df_clean) < 50:\n",
    "        print(\"Error: Not enough data to train after dropping rows with missing targets. Exiting.\")\n",
    "        exit()\n",
    "        \n",
    "    X = df_clean[all_input_features]\n",
    "    y = df_clean[TARGET_COLUMNS]\n",
    "\n",
    "    # --- 2. Preprocessing Pipeline (with Imputation) ---\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns\n",
    "    \n",
    "    # Create pipelines for numeric and categorical features to handle missing values\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')), # Use median for robustness to outliers\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # --- 3. Train-Test Split ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # --- 4. Model Training ---\n",
    "    input_shape = (X_train_processed.shape[1],)\n",
    "    num_outputs = y_train.shape[1]\n",
    "    \n",
    "    model = build_multi_output_model(input_shape, num_outputs)\n",
    "    model.summary() # Print model architecture\n",
    "\n",
    "    print(\"\\n--- Training the multi-output model... ---\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=REDUCE_LR_PATIENCE, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_processed,\n",
    "        y_train,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1 # Show training progress\n",
    "    )\n",
    "    print(\"âœ… Training complete.\")\n",
    "\n",
    "    # --- 5. Evaluation & Reporting ---\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    # y_pred is now a 2D array, so convert it to a DataFrame for easier handling\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=TARGET_COLUMNS, index=y_test.index)\n",
    "    \n",
    "    model_metrics_data = []\n",
    "\n",
    "    print(\"\\n--- Generating reports and plots for each target ---\")\n",
    "    for i, target_name in enumerate(TARGET_COLUMNS):\n",
    "        actuals = y_test[target_name]\n",
    "        predictions = y_pred_df[target_name]\n",
    "        \n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "        \n",
    "        metrics_result = {\n",
    "            'Model Output': target_name,\n",
    "            'Task Type': 'Regression',\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'R2_Score': r2\n",
    "        }\n",
    "        model_metrics_data.append(metrics_result)\n",
    "\n",
    "        # Generate Actual vs. Predicted Plot\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(actuals, predictions, alpha=0.5)\n",
    "        plt.plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], '--r', linewidth=2)\n",
    "        plt.title(f'Actual vs. Predicted for {target_name}')\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plot_filename = f'plot_{target_name}.svg'\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_filename, format='svg')\n",
    "        plt.close()\n",
    "\n",
    "    # Save summary metrics\n",
    "    metrics_df = pd.DataFrame(model_metrics_data)\n",
    "    metrics_filename = 'multi_output_model_performance.xlsx'\n",
    "    metrics_df.to_excel(metrics_filename, index=False)\n",
    "    print(f\"\\nâœ… All performance metrics saved to '{metrics_filename}'.\")\n",
    "    \n",
    "    # Save detailed report with all predictions\n",
    "    detailed_results = X_test.copy()\n",
    "    for col in TARGET_COLUMNS:\n",
    "        detailed_results[f'actual_{col}'] = y_test[col]\n",
    "        detailed_results[f'predicted_{col}'] = y_pred_df[col]\n",
    "        \n",
    "    detailed_results.to_excel('multi_output_detailed_report.xlsx', index=False)\n",
    "    print(f\"âœ… Detailed report saved to 'multi_output_detailed_report.xlsx'.\")\n",
    "\n",
    "    print(\"\\nAll tasks complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
